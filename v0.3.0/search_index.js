var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"background/lsrk/#Low-storage-Runge–Kutta-methods","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"","category":"section"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"LSRK methods are self-starting, with U^(1) = u^n, and then using stage updates of the form","category":"page"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"beginaligned\ndU^(i) = f(U^(i) t + c_i Delta t) + A_i dU^(i-1)\nU^(i+1) = U^(i) + Delta t B_i dU^(i)\nendaligned","category":"page"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"where A_1 = c_1 = 0 (implying dU^(1) = f(u^n t)), with the value at the next step being the N+1th stage value u^n+1 = U^(N+1)).","category":"page"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"This allows the updates to be performed with only two copies of the state vector (so long as f can be evaluated in incrementing form).","category":"page"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"It can be written as an RK scheme with Butcher tableau coefficients defined by the recurrences","category":"page"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"beginaligned\na_ii = 0 \na_ij = B_j + A_j+1 a_ij+1\nb_N = B_N \nb_i = B_i + A_i+1 b_i+1\nendaligned","category":"page"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"or equivalently","category":"page"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"beginaligned\na_jj = 0 \na_ij = a_i-1j + B_i-1 prod_k=j+1^i-1 A_k\nendaligned","category":"page"},{"location":"background/lsrk/","page":"Low-storage Runge–Kutta methods","title":"Low-storage Runge–Kutta methods","text":"with b_j treated analogously as a_N+1j.","category":"page"},{"location":"background/mrrk/#Multirate-Runge-Kutta","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"","category":"section"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"Given a problem with two components that operate at two rates:","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"fracdudt = f_F(ut) + f_S(ut)","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"where f_F is the fast component, and f_S is the slow component.","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"M. Schlegel, O. Knoth, M. Arnold, R. Wolke (2012) defines the following method.","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"Given an outer explicit Runge–Kutta scheme with tableau (abc)","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"We can define the stage values U^(i) = v_i(tau_i) as the solution to the inner ODE problem","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"fracdv_idtau\n  = sum_j=1^i  fraca_ij - a_i-1jc_i - c_i-1  f_S (U^(j) tau_j)\n    + f_F(v_i tau)\nquad tau in tau_i-1 tau","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"where tau_i = t + Delta t c_i, with initial condition v_i(tau_i-1) = U^(i-1). If c_i == c_i-1, we can treat it as a correction step:","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"U^(i) = U^(i-1) + Delta t fracsum_j=1^i (a_ij - a_i-1j)c_i - c_i-1 f_S (U^(j) tau_i)","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"The final summation stage treating analogously to i=N+1, with a_N+1j = b_j and c_N+1 = 1.","category":"page"},{"location":"background/mrrk/#Low-storage","page":"Multirate Runge Kutta","title":"Low-storage","text":"","category":"section"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"If using a low-storage Runge–Kutta method is used as an outer solver, then this reduces to","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"fracdv_idtau\n  =  fracB_i-1c_i - c_i-1 dU_S^(i-1)\n    + f_F(v_i tau)\nquad tau in tau_i-1 tau","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"where","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"dU_S^(i) = f_S(U^(i) tau_i) + A_i dU_S^(i-1)","category":"page"},{"location":"background/mrrk/#Multirate-Infinitesimal-Step-(MIS)","page":"Multirate Runge Kutta","title":"Multirate Infinitesimal Step (MIS)","text":"","category":"section"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"Multirate Infinitesimal Step (MIS) methods (J{\\\"o}rg Wensch, Oswald Knoth, Alexander Galant (2009), Oswald Knoth, Joerg Wensch (2014))","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"beginaligned\nv_i (0)\n  = u^n + sum_j=1^i-1 alpha_ij (U^(j) - u^n)\n\nfracdv_idtau\n  = sum_j=1^i-1 fracgamma_ijd_i Delta t (U^(j) - u^n)\n    + sum_j=1^i fracbeta_ijd_i f_S (U^(j) t + Delta t c_i)\n    + f_F(v_i t^n +  Delta t tilde c_i + fracc_i - tilde c_id_i tau)\nquad tau in 0 Delta t d_i\n\nU^(i) = v_i(Delta t d_i)\nendaligned","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"The method is defined in terms of the lower-triangular matrices alpha, beta and gamma, with d_i = sum_j beta_ij, c_i = (I - alpha - gamma)^-1 d and tilde c = alpha c.","category":"page"},{"location":"background/mrrk/#Wicker-Skamarock","page":"Multirate Runge Kutta","title":"Wicker Skamarock","text":"","category":"section"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"Louis J Wicker, William C Skamarock (1998) and Louis J Wicker, William C Skamarock (2002) define RK2 and RK3 multirate schemes:","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"beginaligned\nv_i (t) = u^n\n\nfracdv_idtau\n  = f_S (U^(i-1) t + Delta t c_i-1)\n    + f_F(v_i tau)\nquad tau in t t+ Delta t c_i \n\nU^(i) = v_i(t + Delta t c_i)\nendaligned","category":"page"},{"location":"background/mrrk/","page":"Multirate Runge Kutta","title":"Multirate Runge Kutta","text":"which corresponds to an MIS method with alpha = gamma = 0 and beta = operatornamediag(c).","category":"page"},{"location":"newtons_method/#Newton's-Method","page":"Newton's Method","title":"Newton's Method","text":"","category":"section"},{"location":"newtons_method/","page":"Newton's Method","title":"Newton's Method","text":"CurrentModule = ClimaTimeSteppers","category":"page"},{"location":"newtons_method/","page":"Newton's Method","title":"Newton's Method","text":"NewtonsMethod","category":"page"},{"location":"newtons_method/#ClimaTimeSteppers.NewtonsMethod","page":"Newton's Method","title":"ClimaTimeSteppers.NewtonsMethod","text":"NewtonsMethod(;\n    max_iters = 1,\n    update_j = UpdateEvery(NewNewtonIteration()),\n    krylov_method = nothing,\n    convergence_checker = nothing,\n    verbose = false,\n)\n\nSolves the equation f(x) = 0, using the Jacobian (or an approximation of the Jacobian) j(x) = f'(x) if it is available. This is done by calling run!(::NewtonsMethod, cache, x, f!, j! = nothing), where f!(f, x) is a function that sets f(x) in-place and, if it is specified, j!(j, x) is a function that sets j(x) in-place. The x passed to Newton's method is modified in-place, and its initial value is used as a starting guess for the root. The cache can be obtained with allocate_cache(::NewtonsMethod, x_prototype, j_prototype = nothing), where x_prototype is similar to x and f(x), and, if it is specified, j_prototype is similar to j(x). In order for j(x) to be invertible, it must be a square matrix, which implies that x and f(x) must be similar to to each other.\n\nLet x[n] denote the value of x on the n-th Newton iteration (with x[0] denoting the initial value of x), and suppose that x[n] is sufficiently close to some root x̂ of f(x) to make the first-order approximation     f(x̂) ≈ f(x[n]) + j(x[n]) * (x̂ - x[n]). Since f(x̂) = 0, the error on the n-th iteration is roughly     x[n] - x̂ ≈ Δx[n], where Δx[n] = j(x[n]) \\ f(x[n]). Newton's method sets x[n + 1] to be the value of x̂ given by this approximation:     x[n + 1] = x[n] - Δx[n].\n\nIf a Krylov method is specified, it gets used to compute the error Δx[n] = j(x[n]) \\ f(x[n]); otherwise, the error is directly computed by calling ldiv!(Δx, j, f). If the Krylov method uses a Jacobian-free JVP (Jacobian-vector product), j_prototype and j! do not need to be specified. When Newton's method uses a Krylov method, it is called a \"Newton-Krylov method\"; furthermore, when the Krylov method uses a Jacobian-free JVP, it is called a \"Jacobian-free Newton-Krylov method\".\n\nIf j_prototype is specified, it should not be an DenseMatrix. If it is, it has to be factorized with lu before ldiv! is called, which requires the allocation of additional memory. Instead, j_prototype should be an object that can directly be passed to ldiv!. For convenience, though, the use of an DenseMatrix is supported. However, Krylov.jl does not provide such support for its preconditioners, so, since the value computed with j! is used as a preconditioner in Krylov methods with a Jacobian-free JVP, using such a Krylov method requires specifying a j_prototype that can be passed to ldiv!.\n\nIf j(x) changes sufficiently slowly, update_j can be changed from UpdateEvery(NewNewtonIteration()) to some other UpdateSignalHandler in order to make the approximation j(x[n]) ≈ j(x₀), where x₀ is a previous value of x[n] (this could even be a value from a previous run! of Newton's method). When Newton's method uses this approximation, it is called the \"chord method\".\n\nIf a convergence checker is provided, it gets used to determine whether to stop iterating on iteration n based on the value x[n] and its error Δx[n]; otherwise, Newton's method iterates from n = 0 to n = max_iters. If the convergence checker determines that x[n] has not converged by the time n = max_iters, a warning gets printed.\n\nIf verbose is set to true, the norms of x[n] and Δx[n] get printed on every iteration. If there is no convergence checker, Δx[n] is not computed on the last iteration, so its final norm is not printed.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#Newton-Krylov-Method","page":"Newton's Method","title":"Newton-Krylov Method","text":"","category":"section"},{"location":"newtons_method/","page":"Newton's Method","title":"Newton's Method","text":"KrylovMethod\nForcingTerm\nConstantForcing\nEisenstatWalkerForcing\nKrylovMethodDebugger\nPrintConditionNumber","category":"page"},{"location":"newtons_method/#ClimaTimeSteppers.KrylovMethod","page":"Newton's Method","title":"ClimaTimeSteppers.KrylovMethod","text":"KrylovMethod(;\n    jacobian_free_jvp = nothing,\n    forcing_term = ConstantForcing(0),\n    type = Val(Krylov.GmresSolver),\n    args = (20,),\n    kwargs = (;),\n    solve_kwargs = (;),\n    disable_preconditioner = false,\n    verbose = false,\n    debugger = nothing,\n)\n\nFinds an approximation Δx[n] ≈ j(x[n]) \\ f(x[n]) for Newton's method such that ‖f(x[n]) - j(x[n]) * Δx[n]‖ ≤ rtol[n] * ‖f(x[n])‖, where rtol[n] is the value of the forcing term on iteration n. This is done by calling run!(::KrylovMethod, cache, Δx, x, f!, f, n, j = nothing), where f is f(x[n]) and, if it is specified, j is either j(x[n]) or an approximation of j(x[n]). The Δx passed to a Krylov method is modified in-place. The cache can be obtained with allocate_cache(::KrylovMethod, x_prototype), where x_prototype is similar to x (and also to Δx and f).\n\nThis is primarily a wrapper for a Krylov.KrylovSolver from Krylov.jl. In allocate_cache, the solver is constructed with solver = type(l, l, args..., Krylov.ktypeof(x_prototype); kwargs...) (note that type must be passed through in a Val struct), where l = length(x_prototype) and Krylov.ktypeof(x_prototype) is a subtype of DenseVector that can be used to store x_prototype. By default, the solver is a Krylov.GmresSolver with a Krylov subspace size of 20 (the default Krylov subspace size for this solver in Krylov.jl). In run!, the solver is run with Krylov.solve!(solver, opj, f; M, ldiv, atol, rtol, verbose, solve_kwargs...).\n\nIn the call to Krylov.solve!, opj is a LinearOperator that represents j(x[n]), which the solver uses by evaluating mul!(jΔx, opj, Δx). If a Jacobian-free JVP (Jacobian-vector product) is specified, it gets used to construct opj and to evaluate the calls to mul!; otherwise, j itself gets used to construct opj, and the calls to mul! simplify to mul!(jΔx, j, Δx).\n\nIf a Jacobian-free JVP and j are both specified, and if disable_preconditioner is set to false, j is treated as an approximation of j(x[n]) and is used as the (left) preconditioner M in order to speed up the solver; otherwise, the preconditioner is simply set to the identity matrix I. The keyword argument ldiv is set to true so that the solver calls ldiv!(Δx′, M, f′) instead of mul!(Δx′, M, f′), where Δx′ and f′ denote internal variables of the solver that roughly correspond to Δx and f. In other words, setting ldiv to true makes the solver treat M as an approximation of j instead of as the inverse of an approximation of j.\n\nThe keyword argument atol is set to 0 because, if it is set to some other value, the inequality ‖f(x[n]) - j(x[n]) * Δx[n]‖ ≤ rtol[n] * ‖f(x[n])‖ changes to ‖f(x[n]) - j(x[n]) * Δx[n]‖ ≤ rtol[n] * ‖f(x[n])‖ + atol, which eliminates any convergence guarantees provided by the forcing term (in order for the Newton-Krylov method to converge, the right-hand side of this inequality must approach 0 as n increases, which cannot happen if atol is not 0).\n\nAll of the arguments and keyword arguments used to construct and run the solver can be modified using args, kwargs, and solve_kwargs. So, the default behavior of this wrapper can be easily overwritten, and any features of Krylov.jl that are not explicitly covered by this wrapper can still be used.\n\nIf verbose is true, the residual ‖f(x[n]) - j(x[n]) * Δx[n]‖ is printed on each iteration of the Krylov method. If a debugger is specified, it is run before the call to Kyrlov.solve!.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.ForcingTerm","page":"Newton's Method","title":"ClimaTimeSteppers.ForcingTerm","text":"ForcingTerm\n\nComputes the value of rtol[n] for a Newton-Krylov method. This is done by calling run!(::ForcingTerm, cache, f, n), which returns rtol[n]. The cache can be obtained with allocate_cache(::ForcingTerm, x_prototype), where x_prototype is similar to f.\n\nFor a detailed discussion of forcing terms and their convergence guarantees, see \"Choosing the Forcing Terms in an Inexact Newton Method\" by S.C. Eisenstat and H.F. Walker (http://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94463.pdf).\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.ConstantForcing","page":"Newton's Method","title":"ClimaTimeSteppers.ConstantForcing","text":"ConstantForcing(rtol)\n\nA ForcingTerm that always returns the value rtol, which must be in the interval [0, 1). If x and f! satisfy certain assumptions, this forcing term guarantees that the Newton-Krylov method will converge linearly with an asymptotic rate of at most rtol. If rtol is 0 (or eps(FT)), this forces the approximation of Δx[n] to be exact (or exact to within machine precision) and guarantees that the Newton-Krylov method will converge quadratically. Note that, although a smaller value of rtol guarantees faster asymptotic convergence, it also leads to a higher probability of oversolving.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.EisenstatWalkerForcing","page":"Newton's Method","title":"ClimaTimeSteppers.EisenstatWalkerForcing","text":"EisenstatWalkerForcing(;\n    initial_rtol = 0.5,\n    γ = 1,\n    α = 2,\n    min_rtol_threshold = 0.1,\n    max_rtol = 0.9,\n)\n\nThe ForcingTerm called \"Choice 2\" in the paper \"Choosing the Forcing Terms in an Inexact Newton Method\" by S.C. Eisenstat and H.F. Walker. The values of initial_rtol, min_rtol_threshold, and max_rtol must be in the interval [0, 1), the value of γ must be in the interval [0, 1], and the value of α must be in the interval (1, 2]. These values can all be tuned to prevent the Newton-Krylov method from oversolving. If x and f! satisfy certain assumptions, this forcing term guarantees that the Newton-Krylov method will converge with order α. Note that, although a larger value of α guarantees a higher convergence order, it also leads to a higher probability of oversolving.\n\nThis forcing term was implemented instead of the one called \"Choice 1\" because it has a significantly simpler implementation–-it only requires the value of ‖f(x[n])‖ to compute rtol[n], whereas \"Choice 1\" also requires the norm of the final residual from the Krylov solver.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.KrylovMethodDebugger","page":"Newton's Method","title":"ClimaTimeSteppers.KrylovMethodDebugger","text":"KrylovMethodDebugger\n\nPrints information about the Jacobian matrix j and the preconditioner M (if it is available) that are passed to a Krylov method. This is done by calling run!(::KrylovMethodDebugger, cache, j, M). The cache can be obtained with allocate_cache(::KrylovMethodDebugger, x_prototype), where x_prototype is similar to x.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.PrintConditionNumber","page":"Newton's Method","title":"ClimaTimeSteppers.PrintConditionNumber","text":"PrintConditionNumber()\n\nPrints the condition number of the Jacobian matrix j, and, if a preconditioner M is available, also prints the condition number of inv(M) * j (i.e., the matrix that actually gets \"inverted\" by the Krylov method). This requires computing dense representations of j and inv(M) * j, which is likely to be significantly slower than the Krylov method itself.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#Jacobian-free-Newton-Krylov-Method","page":"Newton's Method","title":"Jacobian-free Newton-Krylov Method","text":"","category":"section"},{"location":"newtons_method/","page":"Newton's Method","title":"Newton's Method","text":"JacobianFreeJVP\nForwardDiffJVP\nForwardDiffStepSize\nForwardDiffStepSize1\nForwardDiffStepSize2\nForwardDiffStepSize3","category":"page"},{"location":"newtons_method/#ClimaTimeSteppers.JacobianFreeJVP","page":"Newton's Method","title":"ClimaTimeSteppers.JacobianFreeJVP","text":"JacobianFreeJVP\n\nComputes the Jacobian-vector product j(x[n]) * Δx[n] for a Newton-Krylov method without directly using the Jacobian j(x[n]), and instead only using x[n], f(x[n]), and other function evaluations f(x′). This is done by calling run!(::JacobianFreeJVP, cache, jΔx, Δx, x, f!, f). The jΔx passed to a Jacobian-free JVP is modified in-place. The cache can be obtained with allocate_cache(::JacobianFreeJVP, x_prototype), where x_prototype is similar to x (and also to Δx and f).\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.ForwardDiffJVP","page":"Newton's Method","title":"ClimaTimeSteppers.ForwardDiffJVP","text":"ForwardDiffJVP(; default_step = ForwardDiffStepSize3(), step_adjustment = 1)\n\nComputes the Jacobian-vector product using the forward difference approximation j(x) * Δx = (f(x + ε * Δx) - f(x)) / ε, where ε = step_adjustment * default_step(Δx, x).\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.ForwardDiffStepSize","page":"Newton's Method","title":"ClimaTimeSteppers.ForwardDiffStepSize","text":"ForwardDiffStepSize\n\nComputes a default step size for the forward difference approximation of the Jacobian-vector product. This is done by calling default_step(Δx, x), where default_step is a ForwardDiffStepSize.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.ForwardDiffStepSize1","page":"Newton's Method","title":"ClimaTimeSteppers.ForwardDiffStepSize1","text":"ForwardDiffStepSize1()\n\nA ForwardDiffStepSize that is derived based on the notes here: https://web.engr.oregonstate.edu/~webbky/MAE40205020files/Section%204%20Roundoff%20and%20Truncation%20Error.pdf. Although it is not often used with Newton-Krylov methods in practice, it can provide some intuition for for how to set the value of step_adjustment in a ForwardDiffJVP.\n\nThe first-order Taylor series expansion of f(x + ε * Δx) around x is     f(x + ε * Δx) = f(x) + j(x) * (ε * Δx) + e_trunc(x, ε * Δx), where j(x) = f'(x) and e_trunc is the expansion's truncation error. Due to roundoff error, we are unable to directly compute the value of f(x); instead, we can only determine f̂(x), where     f(x) = f̂(x) + e_round(x). Substituting this into the expansion tells us that     f̂(x + ε * Δx) + e_round(x + ε * Δx) =         f̂(x) + e_round(x) + j(x) * (ε * Δx) + e_trunc(x, ε * Δx). Rearranging this gives us the Jacobian-vector product     j(x) * Δx = (f̂(x + ε * Δx) - f̂(x)) / ε - e_trunc(x, ε * Δx) / ε +                  (e_round(x + ε * Δx) - e_round(x)) / ε. So, the normed error of the forward difference approximation of this product is     ‖error‖ = ‖(f̂(x + ε * Δx) - f̂(x)) / ε - j(x) * Δx‖ =              = ‖e_trunc(x, ε * Δx) - e_round(x + ε * Δx) + e_round(x)‖ / ε. We can use the triangle inequality to get the upper bound     ‖error‖ ≤         (‖e_trunc(x, ε * Δx)‖ + ‖e_round(x + ε * Δx)‖ + ‖e_round(x)‖) / ε. If ε is sufficiently small, we can approximate     ‖e_round(x + ε * Δx)‖ ≈ ‖e_round(x)‖. This simplifies the upper bound to     ‖error‖ ≤ (‖e_trunc(x, ε * Δx)‖ + 2 * ‖e_round(x)‖) / ε.\n\nFrom Taylor's theorem (for multivariate vector-valued functions), the truncation error of the first-order expansion is bounded by     ‖e_trunc(x, ε * Δx)‖ ≤ (sup_{x̂ ∈ X} ‖f''(x̂)‖) / 2 * ‖ε * Δx‖^2, where X is a closed ball around x that contains x + ε * Δx (see https://math.stackexchange.com/questions/3478229 for a proof of this). Let us define the value     S = ‖f(x)‖ / sup_{x̂ ∈ X} ‖f''(x̂)‖. By default, we will assume that S ≈ 1, but we will let users pass other values to indicate the \"smoothness\" of f(x) (a large value of S should indicate that the Hessian tensor of f(x) has a small norm compared to f(x) itself). We then have that     ‖e_trunc(x, ε * Δx)‖| ≤ ε^2 / (2 * S) * ‖Δx‖^2 * ‖f(x)‖.\n\nIf only the last bit in each component of f(x) can be altered by roundoff error, then the i-th component of e_round(x) is bounded by     |e_round(x)[i]| ≤ eps(f(x)[i]). More generally, we can assume that there is some constant R (by default, we will assume that R ≈ 1) such that     |e_round(x)[i]| ≤ R * eps(f(x)[i]). We can also make the approximation (which is accurate to within eps(FT))     eps(f(x)[i]) ≈ eps(FT) * |f(x)[i]|. This implies that     |e_round(x)[i]| ≤ R * eps(FT) * |f(x)[i]|. Since this is true for every component of e_round(x) and f(x), we find that     ‖e_round(x)‖ ≤ R * eps(FT) * ‖f(x)‖.\n\nSubstituting the bounds on the truncation and roundoff errors into the bound on the overall error gives us     ‖error‖ ≤ ε / (2 * S) * ‖Δx‖^2 * ‖f(x)‖ + 2 / ε * R * eps(FT) * ‖f(x)‖. Differentiating the right-hand side with respect to ε and setting the result equal to 0 (and noting that the second derivative is always positive) tells us that this upper bound is minimized when     ε = step_adjustment * sqrt(eps(FT)) / ‖Δx‖, where step_adjustment = 2 * sqrt(S * R). By default, we will assume that step_adjustment = 1, but it should be made larger when f is very smooth or has a large roundoff error.\n\nNote that, if we were to replace the forward difference approximation in the derivation above with a central difference approximation, the square root would end up being replaced with a cube root (or, more generally, with an n-th root for a finite difference approximation of order n - 1).\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.ForwardDiffStepSize2","page":"Newton's Method","title":"ClimaTimeSteppers.ForwardDiffStepSize2","text":"ForwardDiffStepSize2()\n\nA ForwardDiffStepSize that is described in the paper \"Jacobian-free Newton–Krylov methods: a survey of approaches and applications\" by D.A. Knoll and D.E. Keyes. According to the paper, this is the step size used by the Fortran package NITSOL.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.ForwardDiffStepSize3","page":"Newton's Method","title":"ClimaTimeSteppers.ForwardDiffStepSize3","text":"ForwardDiffStepSize3()\n\nA ForwardDiffStepSize that is described in the paper \"Jacobian-free Newton–Krylov methods: a survey of approaches and applications\" by D.A. Knoll and D.E. Keyes. According to the paper, this is the average step size one gets when using a certain forward difference approximation for each Jacobian element.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#Convergence-Conditions","page":"Newton's Method","title":"Convergence Conditions","text":"","category":"section"},{"location":"newtons_method/","page":"Newton's Method","title":"Newton's Method","text":"ConvergenceChecker\nConvergenceCondition\nMaximumError\nMaximumRelativeError\nMaximumErrorReduction\nMinimumRateOfConvergence\nMultipleConditions","category":"page"},{"location":"newtons_method/#ClimaTimeSteppers.ConvergenceChecker","page":"Newton's Method","title":"ClimaTimeSteppers.ConvergenceChecker","text":"ConvergenceChecker(;\n    norm_condition,\n    component_condition,\n    condition_combiner,\n    norm = LinearAlgebra.norm,\n)\n\nChecks whether a sequence val[0], val[1], val[2], ... has converged to some limit L, given the errors err[iter] = val[iter] .- L. This is done by calling run!(::ConvergenceChecker, cache, val, err, iter), where val = val[iter] and err = err[iter]. If the value of L is not known, err can be an approximation of err[iter]. The cache for a ConvergenceChecker can be obtained with allocate_cache(::ConvergenceChecker, val_prototype), where val_prototype is similar to val and err.\n\nA ConvergenceChecker can perform two types of checks–-it can check whether norm(val) and norm(err) satisfy some ConvergenceCondition, and it can check whether all the components of abs.(val) and abs.(err) individually satisfy some ConvergenceCondition. These two checks can be combined with either & or |. If one of the checks is not needed, the corresponding ConvergenceCondition can be set to nothing.\n\nInstead of LinearAlgebra.norm, norm can be set to anything that will convert val and err to non-negative scalar values.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.ConvergenceCondition","page":"Newton's Method","title":"ClimaTimeSteppers.ConvergenceCondition","text":"ConvergenceCondition\n\nAn abstract type for objects that can check whether a sequence of non-negative scalar values val[0], val[1], val[2], ... has converged to some limit L, given the errors err[iter] = |val[iter] - L|.\n\nEvery subtype of ConvergenceCondition must define     has_converged(::ConvergenceCondition, cache, val, err, iter). The cache, which is set to nothing by default, may be used to store information from previous iterations that is useful for determining convergence. In order to have access to a cache of some particular type, a subtype of ConvergenceCondition should define     cache_type(::ConvergenceCondition, ::Type{FT}). To specify on which iterations this cache should be updated, it should define     needs_cache_update(::ConvergenceCondition, iter). To specify how the cache should be update on those iterations, it should define     updated_cache(::ConvergenceCondition, cache, val, err, iter).\n\nAlthough cache_type can call promote_type to prevent potential type instability errors, this should be avoided in order to ensure that users write type-stable code.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.MaximumError","page":"Newton's Method","title":"ClimaTimeSteppers.MaximumError","text":"MaximumError(max_err)\n\nChecks whether err[iter] ≤ max_err. Since err[iter] ≥ 0, this can only be true if max_err ≥ 0.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.MaximumRelativeError","page":"Newton's Method","title":"ClimaTimeSteppers.MaximumRelativeError","text":"MaximumRelativeError(max_rel_err)\n\nChecks whether err[iter] ≤ max_rel_err * val[iter]. Since err[iter] ≥ 0 and val[iter] ≥ 0, this can only be true if max_rel_err ≥ 0.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.MaximumErrorReduction","page":"Newton's Method","title":"ClimaTimeSteppers.MaximumErrorReduction","text":"MaximumErrorReduction(max_reduction)\n\nChecks whether err[iter] ≤ max_reduction * err[0] for all iter ≥ 1. Since err[iter] ≥ 0, this can only be true if max_reduction ≥ 0. Also, it must be the case that max_reduction ≤ 1 in order for the sequence to not diverge (i.e., to avoid err[iter] > err[0]).\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.MinimumRateOfConvergence","page":"Newton's Method","title":"ClimaTimeSteppers.MinimumRateOfConvergence","text":"MinimumRateOfConvergence(rate, order = 1)\n\nChecks whether err[iter] ≥ rate * err[iter - 1]^order for all iter ≥ 1. Since err[iter] ≥ 0, this can only be true if rate ≥ 0. Also, if order == 1, it must be the case that rate ≤ 1 in order for the sequence to not diverge (i.e., to avoid err[iter] > err[iter - 1]). In addition, if err[iter] < 1 for all sufficiently large values of iter, it must be the case that order ≥ 1 for the sequence to not diverge.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.MultipleConditions","page":"Newton's Method","title":"ClimaTimeSteppers.MultipleConditions","text":"MultipleConditions(condition_combiner = all, conditions...)\n\nChecks multiple ConvergenceConditions, combining their results with either all or any.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#Update-Signals","page":"Newton's Method","title":"Update Signals","text":"","category":"section"},{"location":"newtons_method/","page":"Newton's Method","title":"Newton's Method","text":"UpdateSignalHandler\nUpdateEvery\nUpdateEveryN\nUpdateSignal\nNewStep\nNewNewtonSolve\nNewNewtonIteration","category":"page"},{"location":"newtons_method/#ClimaTimeSteppers.UpdateSignalHandler","page":"Newton's Method","title":"ClimaTimeSteppers.UpdateSignalHandler","text":"UpdateSignalHandler\n\nUpdates a value upon receiving an appropriate UpdateSignal. This is done by calling run!(::UpdateSignalHandler, cache, ::UpdateSignal, f!, args...), where f! is function such that f!(args...) modifies the desired value in-place. The cache can be obtained with allocate_cache(::UpdateSignalHandler).\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.UpdateEvery","page":"Newton's Method","title":"ClimaTimeSteppers.UpdateEvery","text":"UpdateEvery(update_signal)\n\nAn UpdateSignalHandler that executes the update every time it is run! with update_signal.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.UpdateEveryN","page":"Newton's Method","title":"ClimaTimeSteppers.UpdateEveryN","text":"UpdateEveryN(update_signal, n, reset_n_signal = nothing)\n\nAn UpdateSignalHandler that executes the update every n-th time it is run! with update_signal. If reset_n_signal is specified, then the value of n is reset to 0 every time the signal handler is run! with reset_n_signal.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.UpdateSignal","page":"Newton's Method","title":"ClimaTimeSteppers.UpdateSignal","text":"UpdateSignal\n\nA signal that gets passed to an UpdateSignalHandler whenever a certain operation is performed.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.NewStep","page":"Newton's Method","title":"ClimaTimeSteppers.NewStep","text":"NewStep()\n\nThe signal for a new time step.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.NewNewtonSolve","page":"Newton's Method","title":"ClimaTimeSteppers.NewNewtonSolve","text":"NewNewtonSolve()\n\nThe signal for a new run! of Newton's method, which occurs on every implicit Runge-Kutta stage of the integrator.\n\n\n\n\n\n","category":"type"},{"location":"newtons_method/#ClimaTimeSteppers.NewNewtonIteration","page":"Newton's Method","title":"ClimaTimeSteppers.NewNewtonIteration","text":"NewNewtonIteration()\n\nThe signal for a new iteration of Newton's method.\n\n\n\n\n\n","category":"type"},{"location":"callbacks/#Callbacks","page":"Callbacks","title":"Callbacks","text":"","category":"section"},{"location":"callbacks/","page":"Callbacks","title":"Callbacks","text":"CurrentModule = ClimaTimeSteppers.Callbacks","category":"page"},{"location":"callbacks/","page":"Callbacks","title":"Callbacks","text":"Callbacks","category":"page"},{"location":"callbacks/#ClimaTimeSteppers.Callbacks","page":"Callbacks","title":"ClimaTimeSteppers.Callbacks","text":"ClimaTimeSteppers.Callbacks\n\nA suite of callback functions to be used with the ClimaTimeSteppers.jl ODE solvers.\n\n\n\n\n\n","category":"module"},{"location":"callbacks/#Interfaces","page":"Callbacks","title":"Interfaces","text":"","category":"section"},{"location":"callbacks/","page":"Callbacks","title":"Callbacks","text":"initialize!\nfinalize!","category":"page"},{"location":"callbacks/#ClimaTimeSteppers.Callbacks.initialize!","page":"Callbacks","title":"ClimaTimeSteppers.Callbacks.initialize!","text":"ClimaTimeSteppers.Callbacks.initialize!(f!::F, integrator)\n\nInitialize a callback event for callbacks of type F. By default this does nothing, but can be extended for new callback events.\n\n\n\n\n\n","category":"function"},{"location":"callbacks/#ClimaTimeSteppers.Callbacks.finalize!","page":"Callbacks","title":"ClimaTimeSteppers.Callbacks.finalize!","text":"ClimaTimeSteppers.Callbacks.finalize!(f!::F, integrator)\n\nFinalize a callback event for callbacks of type F. By default this does nothing, but can be extended for new callback events.\n\n\n\n\n\n","category":"function"},{"location":"callbacks/#Callbacks-2","page":"Callbacks","title":"Callbacks","text":"","category":"section"},{"location":"callbacks/","page":"Callbacks","title":"Callbacks","text":"EveryXWallTimeSeconds\nEveryXSimulationTime\nEveryXSimulationSteps","category":"page"},{"location":"callbacks/#ClimaTimeSteppers.Callbacks.EveryXWallTimeSeconds","page":"Callbacks","title":"ClimaTimeSteppers.Callbacks.EveryXWallTimeSeconds","text":"EveryXWallTimeSeconds(f!, Δwt, comm_ctx::ClimaComms.AbstractCommsContext;\n                      atinit=false)\n\nTrigger f!(integrator) every Δwt wallclock seconds.\n\nAn ClimaComms context must be provided to synchronize timing across all ranks.\n\nCallbacks.initialize! and Callbacks.finalize! can be defined for f!.\n\nIf atinit=true, then f!(integrator) will additionally be triggered at initialization, otherwise the first trigger will be after Δwt seconds.\n\n\n\n\n\n","category":"function"},{"location":"callbacks/#ClimaTimeSteppers.Callbacks.EveryXSimulationTime","page":"Callbacks","title":"ClimaTimeSteppers.Callbacks.EveryXSimulationTime","text":"EveryXSimulationTime(f!, Δt;\n                     atinit=false)\n\nTrigger f!(integrator) every Δt simulation time.\n\nCallbacks.initialize! and Callbacks.finalize! can be defined for f!.\n\nIf atinit=true, then f! will additionally be triggered at initialization. Otherwise the first trigger will be after Δt simulation time.\n\n\n\n\n\n","category":"function"},{"location":"callbacks/#ClimaTimeSteppers.Callbacks.EveryXSimulationSteps","page":"Callbacks","title":"ClimaTimeSteppers.Callbacks.EveryXSimulationSteps","text":"EveryXSimulationSteps(f!, Δsteps;\n                     atinit=false)\n\nTrigger f!(integrator) every Δsteps simulation steps.\n\nCallbacks.initialize! and Callbacks.finalize! can be defined for f!.\n\nIf atinit==true, then f! will additionally be triggered at initialization. Otherwise the first trigger will be after Δsteps.\n\n\n\n\n\n","category":"function"},{"location":"background/ark/#Additive-Runge–Kutta","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"","category":"section"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"ARK methods are IMEX (Implicit-Explicit) methods based on splitting the ODE function f(u) = f_L(u) + f_R(t)  where f_L(u) = L u is a linear operator which is treated implicitly, and f_R(u) is the remainder to be treated explicitly. Typically we will be given either the pair (f_R f_L), which we will term the remainder form, or (f f_L) which we will term the full form. ","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"The value on the ith stage U^(i) is","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"U^(i) = u^n + Delta t sum_j=1^i tilde a_ij f_L(U^(j)) \n              + Delta t sum_j=1^i-1 a_ij f_R(U^(j))","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"which can be written as the solution to the linear problem:","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"W U^(i) = hat U^(i)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"where","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"hat U^(i) = u^n + Delta t sum_j=1^i-1 tilde a_ij f_L(U^(j)) \n                                             + Delta t sum_j=1^i-1 a_ij f_R(U^(j))","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"and","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"W = (I - Delta t tilde a_ii L)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"The next step is then defined as","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"u^n+1 = u^n + Delta t sum_i=1^N b_i  f_L(U^(i)) + f_R(U^(i)) ","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"When an iterative solver is used, an initial value bar U^(i) can be chosen by an explicit approximation","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"bar U^(i) = u^n + Delta t sum_j=1^i-1 a_ij  f_L(U^(j)) + f_R(U^(j)) \n            = hat U^(i) + Delta t sum_j=1^i-1 (a_ij - tilde a_ij)  f_L(U^(j)) ","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"By convention, tilde a_11 = 0, so that U^(1) = u^n, and for all other stages the implicit coefficients tilde a_ii are the same. ","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"If the linear operator L is time-invariant and Delta t is constantm, then if using a direct solver, W only needs to be factorized once.","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"Alternatively if an iterative solver is used used, we can write","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"bar U^(i) = u^n + Delta t sum_j=1^i-1 a_ij  f_L(U^(j)) + f_R(U^(j)) \n            = hat U^(i) + Delta t  L sum_j=1^i-1 (a_ij - tilde a_ij)  U^(j)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"at the cost of one evaluation of f_L.","category":"page"},{"location":"background/ark/#Reducing-evaluations-and-storage","page":"Additive Runge–Kutta","title":"Reducing evaluations and storage","text":"","category":"section"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"If the linear operator L is constant, then we are able to avoid evaluating the f_L explicitly.","category":"page"},{"location":"background/ark/#Remainder-form","page":"Additive Runge–Kutta","title":"Remainder form","text":"","category":"section"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"If we are given f_L and f_R, we can avoid storing f_L(U^(j)) by further defining","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"Omega^(i) = sum_j=1^i-1 fractilde a_ijtilde a_ii U^(j)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"and writing","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"hat U^(i) = u^n + Delta t tilde a_ii f_L( Omega^(i) ) + Delta t sum_j=1^i-1 a_ij f_R(U^(j))","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"This can be rewritten into an offset linear problem","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"W V^(i) = hat V^(i)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"where","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"V^(i) = U^(i) + Omega^(i)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"and","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"hat V^(i)\n  = hat U_(i) + (I - Delta t tilde a_ii L)  Omega^(i) \n  = u^n + Omega^(i) + Delta t sum_j=1^i-1 a_ij f_R(U^(j))","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"If using an iterative method, an initial guess is","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"bar V^(i) = bar U^(i) + Omega^(i)\n  = u^n + Delta t sum_j=1^i-1 a_ij f_R(U^(j)) + Omega^(i) + Delta t sum_j=1^i-1 a_ij f_L(U^(j))\n  = hat V^(i) + Delta t L sum_j=1^i-1 a_ij U^(j)","category":"page"},{"location":"background/ark/#Full-form","page":"Additive Runge–Kutta","title":"Full form","text":"","category":"section"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"Similary, if we are given f and f_L, we can avoid storing f_L(U^(j)) by defining","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"Omega^(i) = sum_j=1^i-1 fractilde a_ij - a_ijtilde a_ii U^(j)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"so that we can write","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"hat U^(i) = u^n + Delta t tilde a_ii f_L(Omega^(i)) + Delta t sum_j=1^i-1 a_ij f(U^(j))","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"As above, we can rewrite into an offset linear problem","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"W V^(i) = hat V^(i)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"where","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"V^(i) = U^(i) + Omega^(i)","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"and","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"hat V^(i) \n  = hat U_(i) + (I - Delta t tilde a_ii L)  Omega^(i) \n  = u^n + Omega^(i) + Delta t sum_j=1^i-1 a_ij f(U^(j))","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"If using an iterative method, an initial guess is","category":"page"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"bar V^(i) = bar U^(i) + Omega^(i)\n  = u^n + Delta t sum_j=1^i-1 a_ij f(U^(j)) \n  = hat V^(i) - Omega^(i)","category":"page"},{"location":"background/ark/#References","page":"Additive Runge–Kutta","title":"References","text":"","category":"section"},{"location":"background/ark/","page":"Additive Runge–Kutta","title":"Additive Runge–Kutta","text":"F. X. Giraldo, J. F. Kelly, and E. M. Constantinescu (2013). Implicit-Explicit Formulations of a Three-Dimensional Nonhydrostatic Unified Model of the Atmosphere (NUMA) SIAM Journal on Scientific Computing 35(5), B1162-B1194, doi:10.1137/120876034","category":"page"},{"location":"#ClimaTimeSteppers.jl","page":"ClimaTimeSteppers.jl","title":"ClimaTimeSteppers.jl","text":"","category":"section"},{"location":"","page":"ClimaTimeSteppers.jl","title":"ClimaTimeSteppers.jl","text":"ClimaTimeSteppers.jl is a suite of ordinary differential equation (ODE) solvers for use as time-stepping methods in a partial differential equation (PDE) solver, such as ClimateMachine.jl. They are specifically written to support distributed and GPU computation, while minimising the memory footprint.","category":"page"},{"location":"","page":"ClimaTimeSteppers.jl","title":"ClimaTimeSteppers.jl","text":"ClimaTimeSteppers.jl is built on top of DiffEqBase.jl, and aims to be compatible with the DifferentialEquations.jl ecosystem.","category":"page"},{"location":"","page":"ClimaTimeSteppers.jl","title":"ClimaTimeSteppers.jl","text":"ClimaTimeSteppers","category":"page"},{"location":"#ClimaTimeSteppers","page":"ClimaTimeSteppers.jl","title":"ClimaTimeSteppers","text":"ClimaTimeSteppers\n\nOrdinary differential equation solvers\n\nJuliaDiffEq terminology:\n\nFunction: the right-hand side function df/dt.\nby default, a function gets wrapped in an ODEFunction\ndefine new IncrementingODEFunction to support incrementing function calls.\nProblem: Function, initial u, time span, parameters and options\ndu/dt = f(u,p,t) = fL(u,p,t)  + fR(u,p,t)\nfR(u,p,t) == f(u.p,t) - fL(u,p,t) fL(u,,) == A*u for some A (matrix free)\nSplitODEProlem(fL, fR)\n\nODEProblem from OrdinaryDiffEq.jl\nuse jac option to ODEFunction for linear + full IMEX (https://docs.sciml.ai/latest/features/performanceoverloads/#odeexplicit_jac-1)\nSplitODEProblem for linear + remainder IMEX\nMultirateODEProblem for true multirate\nAlgorithm: small objects (often singleton) which indicate what algorithm + options (e.g. linear solver type)\ndefine new abstract DistributedODEAlgorithm, algorithms in this pacakge will be subtypes of this\ndefine new Multirate for multirate solvers\nIntegrator: contains everything necessary to solve. Used as:\ndefine new DistributedODEIntegrator for solvers in this package\ninit(prob, alg, options...) => integrator   step!(int) => runs single step   solve!(int) => runs it to end   solve(prob, alg, options...) => init + solve!\nSolution (not implemented): contains the \"solution\" to the ODE.\n\n\n\n\n\n","category":"module"},{"location":"background/ssprk/#Strong-Stability-Preserving-Runge–Kutta-methods","page":"Strong Stability Preserving Runge–Kutta methods","title":"Strong Stability Preserving Runge–Kutta methods","text":"","category":"section"},{"location":"background/ssprk/","page":"Strong Stability Preserving Runge–Kutta methods","title":"Strong Stability Preserving Runge–Kutta methods","text":"SSPRK methods are self-starting, with U^(1) = u^n, and subsequent stage updates of the form","category":"page"},{"location":"background/ssprk/","page":"Strong Stability Preserving Runge–Kutta methods","title":"Strong Stability Preserving Runge–Kutta methods","text":"beginaligned\nU^(i+1) = A_i1 u^n + A_i2 U^(i) + Delta t B_i f(U^(i) t + c_i Delta t)\nendaligned","category":"page"},{"location":"background/ssprk/","page":"Strong Stability Preserving Runge–Kutta methods","title":"Strong Stability Preserving Runge–Kutta methods","text":"with the value at the next step being the N+1th stage value u^n+1 = U^(N+1)).","category":"page"},{"location":"background/ssprk/","page":"Strong Stability Preserving Runge–Kutta methods","title":"Strong Stability Preserving Runge–Kutta methods","text":"This allows the updates to be performed with only three copies of the state vector (storing u^n, U^(i) and f(U^(i)t)).","category":"page"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"CurrentModule = ClimaTimeSteppers","category":"page"},{"location":"algorithms/#Interface-and-OrdinaryDiffEq-compatibility","page":"Algorithms","title":"Interface and OrdinaryDiffEq compatibility","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"allocate_cache\nrun!\nForwardEulerODEFunction","category":"page"},{"location":"algorithms/#ClimaTimeSteppers.allocate_cache","page":"Algorithms","title":"ClimaTimeSteppers.allocate_cache","text":"allocate_cache(alg, prototypes...)\n\nAllocate a cache that the algorithm will use during run!, given some values that are similar to values that need to be cached.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#ClimaTimeSteppers.run!","page":"Algorithms","title":"ClimaTimeSteppers.run!","text":"run!(alg, cache, args...)\n\nRun the algorithm for the given arguments, using the cache to store intermediate values. Depending on the algorithm, the input arguments may also get modified.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#ClimaTimeSteppers.ForwardEulerODEFunction","page":"Algorithms","title":"ClimaTimeSteppers.ForwardEulerODEFunction","text":"ForwardEulerODEFunction(f; jac_prototype, Wfact, tgrad)\n\nAn ODE function wrapper where f(un, u, p, t, dt) provides a forward Euler update\n\nun .= u .+ dt * f(u, p, t)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#IMEX-ARK-methods","page":"Algorithms","title":"IMEX ARK methods","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"IMEXARKAlgorithm\nmake_IMEXARKTableau","category":"page"},{"location":"algorithms/#ClimaTimeSteppers.IMEXARKAlgorithm","page":"Algorithms","title":"ClimaTimeSteppers.IMEXARKAlgorithm","text":"IMEXARKAlgorithm <: DistributedODEAlgorithm\n\nA generic implementation of an IMEX ARK algorithm that can handle arbitrary Butcher tableaus and problems specified using either ForwardEulerODEFunctions or regular ODEFunctions.\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.make_IMEXARKTableau","page":"Algorithms","title":"ClimaTimeSteppers.make_IMEXARKTableau","text":"make_IMEXARKTableau(; a_exp, b_exp, c_exp, a_imp, b_imp, c_imp)\n\nGenerates an IMEXARKAlgorithm type from an IMEX ARK Butcher tableau. Only a_exp and a_imp are required arguments; the default values for b_exp and b_imp assume that the algorithm is FSAL (first same as last), and the default values for c_exp and c_imp assume that the algorithm is internally consistent.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The convergence orders of the provided methods are verified using test cases from ARKode. Plots of the solutions to these test cases, the errors of these solutions, and the convergence orders with respect to dt are shown below.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"using Pkg\nPkg.activate(\"../../test\")\nPkg.instantiate()\ninclude(\"../../test/convergence.jl\")\nPkg.activate(\".\")","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Plots for ark_analytic: (Image: ) (Image: ) (Image: )","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Plots for ark_analytic_nonlin: (Image: ) (Image: ) (Image: )","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Plots for ark_analytic_sys: (Image: ) (Image: ) (Image: )","category":"page"},{"location":"algorithms/#Low-Storage-Runge–Kutta-(LSRK)-methods","page":"Algorithms","title":"Low-Storage Runge–Kutta (LSRK) methods","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Low-storage Runger–Kutta methods reduce the number stages that need to be stored. The methods below require only one additional storage vector.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"An IncrementingODEProblem must be used.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LowStorageRungeKutta2N\nLSRK54CarpenterKennedy\nLSRK144NiegemannDiehlBusch\nLSRKEulerMethod","category":"page"},{"location":"algorithms/#ClimaTimeSteppers.LowStorageRungeKutta2N","page":"Algorithms","title":"ClimaTimeSteppers.LowStorageRungeKutta2N","text":"LowStorageRungeKutta2N <: DistributedODEAlgorithm\n\nA class of low-storage Runge-Kutta algorithms, which use only one additional copy of the state vector u (often referred to as 2N schemes).\n\nThe available implementations are:\n\nLSRKEulerMethod\nLSRK54CarpenterKennedy\nLSRK144NiegemannDiehlBusch\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.LSRK54CarpenterKennedy","page":"Algorithms","title":"ClimaTimeSteppers.LSRK54CarpenterKennedy","text":"LSRK54CarpenterKennedy()\n\nThe 4th-order, 5-stage [LowStorageRungeKutta2N])(ref) scheme from Solution 3 of Mark H Carpenter, Christopher A Kennedy (1994).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.LSRK144NiegemannDiehlBusch","page":"Algorithms","title":"ClimaTimeSteppers.LSRK144NiegemannDiehlBusch","text":"LSRK144NiegemannDiehlBusch()\n\nThe 4th-order, 14-stage, [LowStorageRungeKutta2N])(ref) scheme of Jens Niegemann, Richard Diehl, Kurt Busch (2012) with optimized stability region\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.LSRKEulerMethod","page":"Algorithms","title":"ClimaTimeSteppers.LSRKEulerMethod","text":"LSRKEulerMethod()\n\nAn implementation of explicit Euler method using LowStorageRungeKutta2N infrastructure. This is mainly for debugging.\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Strong-Stability-Preserving-Runge–Kutta-(SSPRK)-methods","page":"Algorithms","title":"Strong Stability-Preserving Runge–Kutta (SSPRK) methods","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"StrongStabilityPreservingRungeKutta\nSSPRK22Heuns\nSSPRK22Ralstons\nSSPRK33ShuOsher\nSSPRK34SpiteriRuuth","category":"page"},{"location":"algorithms/#ClimaTimeSteppers.StrongStabilityPreservingRungeKutta","page":"Algorithms","title":"ClimaTimeSteppers.StrongStabilityPreservingRungeKutta","text":"StrongStabilityPreservingRungeKutta <: DistributedODEAlgorithm\n\nA class of Strong Stability Preserving Runge–Kutta methods. These require two additional copies of the state vector.\n\nThe available implementations are:\n\nSSPRK22Heuns\nSSPRK22Ralstons\nSSPRK33ShuOsher\nSSPRK34SpiteriRuuth\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.SSPRK22Heuns","page":"Algorithms","title":"ClimaTimeSteppers.SSPRK22Heuns","text":"SSPRK22Heuns()\n\nThe second-order, 2-stage, strong-stability-preserving, Runge–Kutta scheme of Chi-Wang Shu, Stanley Osher (1988), also known as Heun's method (Heun1900.\n\nExact choice of coefficients from wikipedia page for Heun's method :)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.SSPRK22Ralstons","page":"Algorithms","title":"ClimaTimeSteppers.SSPRK22Ralstons","text":"SSPRK22Ralstons()\n\nThe second-order, 2-stage, strong-stability-preserving, Runge–Kutta scheme of Chi-Wang Shu, Stanley Osher (1988), also known as Ralstons's method (Anthony Ralston (1962).\n\nExact choice of coefficients from wikipedia page for Heun's method :)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.SSPRK33ShuOsher","page":"Algorithms","title":"ClimaTimeSteppers.SSPRK33ShuOsher","text":"SSPRK33ShuOsher()\n\nThe third-order, 3-stage, strong-stability-preserving, Runge–Kutta scheme of Chi-Wang Shu, Stanley Osher (1988).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.SSPRK34SpiteriRuuth","page":"Algorithms","title":"ClimaTimeSteppers.SSPRK34SpiteriRuuth","text":"SSPRK34SpiteriRuuth()\n\nThe third-order, 4-stage, strong-stability-preserving, Runge–Kutta scheme of Raymond J Spiteri, Steven J Ruuth (2002).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Additive-Runge–Kutta-(ARK)-methods","page":"Algorithms","title":"Additive Runge–Kutta (ARK) methods","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ARK methods are IMEX (Implicit-Explicit) methods based on splitting the ODE function into a linear and remainder components:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"fracdudt = Lu + f_R(ut)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where the linear part is solved implicitly. All the algorithms below take a linsolve argument to specify the linear solver to be used. See the linsolve specification of DifferentialEquations.jl.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Currently ARK methods require a SplitODEProblem.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"AdditiveRungeKutta\nARK1ForwardBackwardEuler\nARK2ImplicitExplicitMidpoint\nARK2GiraldoKellyConstantinescu\nARK437L2SA1KennedyCarpenter\nARK548L2SA2KennedyCarpenter","category":"page"},{"location":"algorithms/#ClimaTimeSteppers.AdditiveRungeKutta","page":"Algorithms","title":"ClimaTimeSteppers.AdditiveRungeKutta","text":"AdditiveRungeKutta\n\nIMEX (IMplicit-EXplicit) algorithms using ARK (Additively-partitioned Runge-Kutta) methods.\n\nThe available implementations are:\n\nARK1ForwardBackwardEuler\nARK2ImplicitExplicitMidpoint\nARK2GiraldoKellyConstantinescu\nARK437L2SA1KennedyCarpenter\nARK548L2SA2KennedyCarpenter\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.ARK1ForwardBackwardEuler","page":"Algorithms","title":"ClimaTimeSteppers.ARK1ForwardBackwardEuler","text":"ARK1ForwardBackwardEuler(linsolve)\n\nA first-order-accurate two-stage additive Runge–Kutta scheme of Uri M. Ascher, Steven J. Ruuth, Raymond J. Spiteri (1997), combining a forward Euler explicit step with a backward Euler implicit correction.\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.ARK2ImplicitExplicitMidpoint","page":"Algorithms","title":"ClimaTimeSteppers.ARK2ImplicitExplicitMidpoint","text":"ARK2ImplicitExplicitMidpoint(linsolve)\n\nA second-order, two-stage additive Runge–Kutta scheme Uri M. Ascher, Steven J. Ruuth, Raymond J. Spiteri (1997), combining the implicit and explicit midpoint methods.\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.ARK2GiraldoKellyConstantinescu","page":"Algorithms","title":"ClimaTimeSteppers.ARK2GiraldoKellyConstantinescu","text":"ARK2GiraldoKellyConstantinescu(linsolve; paperversion=false)\n\nThe second-order, 3-stage additive Runge–Kutta scheme of GKC2013.\n\nIf the keyword paperversion=true is used, the coefficients from the paper are used. Otherwise it uses coefficients that make the scheme (much) more stable but less accurate\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.ARK437L2SA1KennedyCarpenter","page":"Algorithms","title":"ClimaTimeSteppers.ARK437L2SA1KennedyCarpenter","text":"ARK437L2SA1KennedyCarpenter(linsolve)\n\nThe fourth-order, 7-stage additive Runge–Kutta scheme ARK4(3)7L[2]SA₁ of Christopher A Kennedy, Mark H Carpenter (2019).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.ARK548L2SA2KennedyCarpenter","page":"Algorithms","title":"ClimaTimeSteppers.ARK548L2SA2KennedyCarpenter","text":"ARK548L2SA2KennedyCarpenter(linsolve)\n\nThe fifth-order, 8-stage additive Runge–Kutta scheme ARK5(4)8L[2]SA₂ of Christopher A Kennedy, Mark H Carpenter (2019).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ARS","page":"Algorithms","title":"ARS","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"TODO: ARS111, ARS121, and ARS343 should probably be types, and not constants, so that we can properly document them.","category":"page"},{"location":"algorithms/#Multirate","page":"Algorithms","title":"Multirate","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Multirate","category":"page"},{"location":"algorithms/#ClimaTimeSteppers.Multirate","page":"Algorithms","title":"ClimaTimeSteppers.Multirate","text":"Multirate(fast, slow)\n\nA multirate Runge–Kutta scheme, combining fast and slow algorithms\n\nslow can be any algorithm providing methods for the following functions\n\ninit_inner(prob, outercache, dt)\nupdate_inner!(innerinteg, outercache, f_slow, u, p, t, dt, stage)\n\nAlgorithms which currently support this are:\n\nLowStorageRungeKutta2N\nMultirateInfinitesimalStep\nWickerSkamarockRungeKutta\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Multirate-Infinitesimal-Step","page":"Algorithms","title":"Multirate Infinitesimal Step","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"MultirateInfinitesimalStep\nMIS2\nMIS3C\nMIS4\nMIS4a\nTVDMISA\nTVDMISB","category":"page"},{"location":"algorithms/#ClimaTimeSteppers.MultirateInfinitesimalStep","page":"Algorithms","title":"ClimaTimeSteppers.MultirateInfinitesimalStep","text":"MultirateInfinitesimalStep\n\nMultirate Infinitesimal Step (MIS) methods of J{\\\"o}rg Wensch, Oswald Knoth, Alexander Galant (2009) and Oswald Knoth, Joerg Wensch (2014).\n\nThe available implementations are:\n\nMIS2\nMIS3C\nMIS4\nMIS4a\nTVDMISA\nTVDMISB\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.MIS2","page":"Algorithms","title":"ClimaTimeSteppers.MIS2","text":"MIS2()\n\nThe MIS2 Multirate Infinitesimal Step (MIS) method of Oswald Knoth, Joerg Wensch (2014).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.MIS3C","page":"Algorithms","title":"ClimaTimeSteppers.MIS3C","text":"MIS3C()\n\nThe MIS3C Multirate Infinitesimal Step (MIS) method of Oswald Knoth, Joerg Wensch (2014).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.MIS4","page":"Algorithms","title":"ClimaTimeSteppers.MIS4","text":"MIS4()\n\nThe MIS4 Multirate Infinitesimal Step (MIS) method of Oswald Knoth, Joerg Wensch (2014).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.MIS4a","page":"Algorithms","title":"ClimaTimeSteppers.MIS4a","text":"MIS4a()\n\nThe MIS4a Multirate Infinitesimal Step (MIS) method of Oswald Knoth, Joerg Wensch (2014).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.TVDMISA","page":"Algorithms","title":"ClimaTimeSteppers.TVDMISA","text":"TVDMISA()\n\nThe TVDMISA Total Variation Diminishing (TVD) Multirate Infinitesimal Step (MIS) method of Oswald Knoth, Joerg Wensch (2014).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.TVDMISB","page":"Algorithms","title":"ClimaTimeSteppers.TVDMISB","text":"TVDMISB()\n\nThe TVDMISB Total Variation Diminishing (TVD) Multirate Infinitesimal Step (MIS) method of Oswald Knoth, Joerg Wensch (2014).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Wicker–Skamarock","page":"Algorithms","title":"Wicker–Skamarock","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"WickerSkamarockRungeKutta\nWSRK2\nWSRK3","category":"page"},{"location":"algorithms/#ClimaTimeSteppers.WickerSkamarockRungeKutta","page":"Algorithms","title":"ClimaTimeSteppers.WickerSkamarockRungeKutta","text":"WickerSkamarockRungeKutta <: DistributedODEAlgorithm\n\nClass of multirate algorithms developed in Louis J Wicker, William C Skamarock (1998) and Louis J Wicker, William C Skamarock (2002), which can be used as slow methods in Multirate.\n\nThese require two additional copies of the state vector u.\n\nAvailable implementations are:\n\nWSRK2\nWSRK3\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.WSRK2","page":"Algorithms","title":"ClimaTimeSteppers.WSRK2","text":"WSRK2()\n\nThe 2 stage, 2nd order RK2 scheme of Louis J Wicker, William C Skamarock (1998).\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#ClimaTimeSteppers.WSRK3","page":"Algorithms","title":"ClimaTimeSteppers.WSRK3","text":"WSRK3()\n\nThe 3 stage, 2nd order (3rd order for linear problems) RK3 scheme of Louis J Wicker, William C Skamarock (2002).\n\n\n\n\n\n","category":"type"}]
}
