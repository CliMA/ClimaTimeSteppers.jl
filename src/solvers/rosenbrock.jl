#=
## Introduction to Rosenbrock Methods

An s-stage diagonally implicit Runge-Kutta (DIRK) method for solving
âˆ‚u/âˆ‚t = f(u, t) is specified by a lower triangular sÃ—s matrix a (whose values
are called "internal coefficients"), a vector b of length s (whose values are
called "weights"), and a vector c of length s (whose values are called
"abcissae" or "nodes").
Given the state u at time t, the state u_next at time t + Î”t is given by
    u_next := u + Î”t * âˆ‘_{i=1}^s báµ¢ * f(Uáµ¢, t + Î”t * cáµ¢), where
    Uáµ¢ := u + Î”t * âˆ‘_{j=1}^i aáµ¢â±¼ * f(Uâ±¼, t + Î”t * câ±¼).
In order to transform this DIRK method into a Rosenbrock method, we must assume
that it is "internally consistent", which means that
    cáµ¢ := âˆ‘_{j=1}^i aáµ¢â±¼.

First, we will simplify our notation by defining
    Táµ¢ := t + Î”t * âˆ‘_{j=1}^i aáµ¢â±¼ and
    Fáµ¢ := f(Uáµ¢, Táµ¢).
This simplifies our previous expressions to
    u_next = u + Î”t * âˆ‘_{i=1}^s báµ¢ * Fáµ¢ and
    Uáµ¢ = u + Î”t * âˆ‘_{j=1}^i aáµ¢â±¼ * Fâ±¼.
Next, we will define
    UÌ‚áµ¢ := u + Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼ * Fâ±¼ and
    TÌ‚áµ¢ := t + Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼.
This implies that
    Uáµ¢ = UÌ‚áµ¢ + Î”t * aáµ¢áµ¢ * Fáµ¢ and
    Táµ¢ = TÌ‚áµ¢ + Î”t * aáµ¢áµ¢.
Substituting this into the definition of Fáµ¢ gives us
    Fáµ¢ = f(UÌ‚áµ¢ + Î”t * aáµ¢áµ¢ * Fáµ¢, TÌ‚áµ¢ + Î”t * aáµ¢áµ¢).
Approximating the value of f with a first-order Taylor series expansion around
UÌ‚áµ¢ and TÌ‚áµ¢ gives us
    Fáµ¢ â‰ˆ f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J(UÌ‚áµ¢, TÌ‚áµ¢) * aáµ¢áµ¢ * Fáµ¢ + Î”t * fÌ‡(UÌ‚áµ¢, TÌ‚áµ¢) * aáµ¢áµ¢, where
    J(u, t) := âˆ‚f/âˆ‚u(u, t) and
    fÌ‡(u, t) := âˆ‚f/âˆ‚t(u, t).
This is roughly equivalent to running a single Newton iteration to solve for Fáµ¢,
starting with an initial guess of f(UÌ‚áµ¢, TÌ‚áµ¢) (or, equivalently, to solve for Uáµ¢,
starting with an initial guess of UÌ‚áµ¢).

In order to obtain a Rosenbrock method, we must modify this equation for Fáµ¢.
By using only a single Newton iteration, we break the assumptions that provide
guarantees about the convergence of the DIRK method.
To remedy this, we introduce an additional lower triangular sÃ—s matrix of
coefficients Î³, and we redefine Fáµ¢ to be the solution to
    Fáµ¢ = f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J(UÌ‚áµ¢, TÌ‚áµ¢) * âˆ‘_{j=1}^i Î³áµ¢â±¼ * Fâ±¼ +
         Î”t * fÌ‡(UÌ‚áµ¢, TÌ‚áµ¢) * âˆ‘_{j=1}^i Î³áµ¢â±¼.
In other words, we account for the higher-order terms dropped by the first-order
Taylor series expansion by taking carefully chosen linear combinations of Fâ±¼.
Since this effectively replaces aáµ¢áµ¢ with entries in Î³, the diagonal entries can
now be omitted from a, making it strictly lower triangular.

For a "modified Rosenbrock method", the coefficients are chosen so that
J(UÌ‚áµ¢, TÌ‚áµ¢) and fÌ‡(UÌ‚áµ¢, TÌ‚áµ¢) can be approximated by their values at time t, J(u, t)
and fÌ‡(u, t).
For a "W-method", the approximation can be even coarser; e.g., the values at a
previous timestep, or the values at a precomputed reference state, or even
approximations that do not correspond to any particular state.
So, we will modify the last equation by replacing J(UÌ‚áµ¢, TÌ‚áµ¢) and fÌ‡(UÌ‚áµ¢, TÌ‚áµ¢) with
some approximations J and fÌ‡ (whose values must satisfy the constraints of the
specific Rosenbrock method being used), giving us
    Fáµ¢ = f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J * âˆ‘_{j=1}^i Î³áµ¢â±¼ * Fâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼.
Solving this equation for Fáµ¢ lets us redefine
    Fáµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ +
              Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ).
Since multiplying âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ by J is more computationally expensive
than subtracting it from another value, we will rewrite this as
    Fáµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) + Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ +
              Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ) - Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼.

So, in summary, a Rosenbrock method is defined by some lower triangular sÃ—s
matrix Î³, some strictly lower triangular sÃ—s matrix a, some vector b of length
s, and some approximations J and fÌ‡ of âˆ‚f/âˆ‚u(UÌ‚áµ¢, TÌ‚áµ¢) and âˆ‚f/âˆ‚t(UÌ‚áµ¢, TÌ‚áµ¢), which
are all used to compute
    u_next := u + Î”t * âˆ‘_{i=1}^s báµ¢ * Fáµ¢, where
    UÌ‚áµ¢ := u + Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼ * Fâ±¼,
    TÌ‚áµ¢ := t + Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼, and
    Fáµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) + Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ +
              Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ) - Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼.

## Converting to Matrix Form

To simplify our further reformulations, we will convert our definitions to
matrix form.

First, we will reduce the number of matrix equations we need to analyze by
defining
    aÌ‚áµ¢â±¼ := i < s ? aâ‚áµ¢â‚Šâ‚â‚â±¼ : bâ±¼ and
    UÌ‚âºáµ¢ := i < s ? UÌ‚áµ¢â‚Šâ‚ : u_next.
We can then redefine
    u_next := UÌ‚âºâ‚› and
    UÌ‚áµ¢ := i == 1 ? u : UÌ‚âºáµ¢â‚‹â‚, where
    UÌ‚âºáµ¢ := u + Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * Fâ±¼.

The equations we will convert to matrix form are
    UÌ‚âºáµ¢ = u + Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * Fâ±¼ and
    Fáµ¢ = f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J * âˆ‘_{j=1}^i Î³áµ¢â±¼ * Fâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼.
Rewriting these with an explicit element index n âˆˆ 1:N (where N = length(u))
gives us
    UÌ‚âºáµ¢[n] = u[n] + Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * Fâ±¼[n] and
    Fáµ¢[n] = f(UÌ‚áµ¢, TÌ‚áµ¢)[n] + Î”t * âˆ‘_{m=1}^N J[n,m] * âˆ‘_{j=1}^i Î³áµ¢â±¼ * Fâ±¼[m] +
            Î”t * fÌ‡[n] * âˆ‘_{j=1}^i Î³áµ¢â±¼.
Now, we will formally define the vectors and matrices
    ğŸ™ âˆˆ â„Ë¢     | ğŸ™áµ¢ := 1,
    u âˆˆ â„á´º     | uâ‚™ := u[n],
    fÌ‡ âˆˆ â„á´º     | fÌ‡â‚™ := fÌ‡[n], and
    UÌ‚âº âˆˆ â„á´ºÃ—â„Ë¢ | UÌ‚âºâ‚™áµ¢ := UÌ‚âºáµ¢[n],
    F âˆˆ â„á´ºÃ—â„Ë¢  | Fâ‚™áµ¢ := Fáµ¢[n],
    FÌ‚ âˆˆ â„á´ºÃ—â„Ë¢  | FÌ‚â‚™áµ¢ := f(UÌ‚áµ¢, TÌ‚áµ¢)[n],
    J âˆˆ â„á´ºÃ—â„á´º  | Jâ‚™â‚˜ := J[n,m].
If J and fÌ‡ are different for each stage, they may be replaced with tensors in
the following manipulations.
We then have that
    UÌ‚âºâ‚™áµ¢ = uâ‚™ * ğŸ™áµ¢ + Î”t * âˆ‘_{j=1}^i Fâ‚™â±¼ * aÌ‚áµ¢â±¼ and
    Fâ‚™áµ¢ = FÌ‚â‚™áµ¢ + Î”t * âˆ‘_{m=1}^N Jâ‚™â‚˜ * âˆ‘_{j=1}^i Fâ‚˜â±¼ * Î³áµ¢â±¼ +
          Î”t * fÌ‡â‚™ * âˆ‘_{j=1}^i ğŸ™â±¼ * Î³áµ¢â±¼.
We can rewrite this as
    UÌ‚âºâ‚™áµ¢ = uâ‚™ * (ğŸ™áµ€)áµ¢ + Î”t * âˆ‘_{j=1}^s Fâ‚™â±¼ * (aÌ‚áµ€)â±¼áµ¢ and
    Fâ‚™áµ¢ = FÌ‚â‚™áµ¢ + Î”t * âˆ‘_{m=1}^N Jâ‚™â‚˜ * âˆ‘_{j=1}^s Fâ‚˜â±¼ * (Î³áµ€)â±¼áµ¢ +
          Î”t * fÌ‡â‚™ * âˆ‘_{j=1}^s (ğŸ™áµ€)â±¼ * (Î³áµ€)â±¼áµ¢.
Combining matrix-matrix and matrix-vector products gives us
    UÌ‚âºâ‚™áµ¢ = uâ‚™ * (ğŸ™áµ€)áµ¢ + Î”t * (F * aÌ‚áµ€)â‚™áµ¢ and
    Fâ‚™áµ¢ = FÌ‚â‚™áµ¢ + Î”t * (J * F * Î³áµ€)â‚™áµ¢ + Î”t * fÌ‡â‚™ * (Î³ * ğŸ™)áµ¢.
Dropping the indices then tells us that
    UÌ‚âº = u âŠ— ğŸ™áµ€ + Î”t * F * aÌ‚áµ€ and
    F = FÌ‚ + Î”t * J * F * Î³áµ€ + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
To rewrite this in a way that more directly corresponds to our last formulation,
we will define the functions diag() and lower(), such that, for any lower
triangular matrix M,
    M = diag(M) + lower(M), where
    diag(M)áµ¢â±¼ := i == j ? Máµ¢â±¼ : 0 and
    lower(M)áµ¢â±¼ := i > j ? Máµ¢â±¼ : 0.
This lets us rewrite the equation for F as
    F * (I + diag(Î³)â»Â¹ * lower(Î³))áµ€ -
    Î”t * J * F * (I + diag(Î³)â»Â¹ * lower(Î³))áµ€ * diag(Î³)áµ€ =
        = FÌ‚ + F * (diag(Î³)â»Â¹ * lower(Î³))áµ€ + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.

We will now use these matrix equations to define two reformulations: one that
optimizes performance by eliminating the subtraction after the linear solve, and
one that enables limiters by appropriately modifying the value being subtracted.

## Eliminating the Subtraction

Let us define a new NÃ—s matrix
    K := F * Î³áµ€.
We then have that
    F = K * (Î³â»Â¹)áµ€.
This allows us to rewrite the matrix equations as
    UÌ‚âº = u âŠ— ğŸ™áµ€ + Î”t * K * (aÌ‚ * Î³â»Â¹)áµ€ and
    K * (Î³â»Â¹)áµ€ = FÌ‚ + Î”t * J * K + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
Since Î³ is lower triangular,
    Î³â»Â¹ = diag(Î³â»Â¹) + lower(Î³â»Â¹) = diag(Î³)â»Â¹ + lower(Î³â»Â¹).
This lets us rewrite the equation for K as
    K * (diag(Î³)â»Â¹ + lower(Î³â»Â¹))áµ€ = FÌ‚ + Î”t * J * K + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
Multiplying through on the right by diag(Î³)áµ€ and rearranging gives us
    K - Î”t * J * K * diag(Î³)áµ€ =
        (FÌ‚ - K * lower(Î³â»Â¹)áµ€ + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€) * diag(Î³)áµ€.
Taking this and the equation for UÌ‚âº back out of matrix form gives us
    UÌ‚âºáµ¢ = u + Î”t * âˆ‘_{j=1}^i (aÌ‚ * Î³â»Â¹)áµ¢â±¼ * Kâ±¼ and
    Káµ¢ - Î”t * Î³áµ¢áµ¢ * J * Káµ¢ =
        = Î³áµ¢áµ¢ *
          (f(UÌ‚áµ¢, TÌ‚áµ¢) - âˆ‘_{j=1}^{i-1} (Î³â»Â¹)áµ¢â±¼ * Kâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼).
Thus, to optimize performance, we can redefine
    UÌ‚âºáµ¢ := u + Î”t * âˆ‘_{j=1}^i (aÌ‚ * Î³â»Â¹)áµ¢â±¼ * Kâ±¼, where
    Káµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * Î³áµ¢áµ¢ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) - âˆ‘_{j=1}^{i-1} (Î³â»Â¹)áµ¢â±¼ * Kâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ).

## Enabling Limiters

In the previous section, we changed the temporary variable from Fáµ¢, which is an
approximation of f(Uáµ¢, Táµ¢), to Káµ¢, which is a linear combination of previously
computed values of Fáµ¢.
Now, we will change the temporary variable to Váµ¢, so that the approximations of
Uáµ¢ and u_next will be linear combinations of previously computed values of Váµ¢.
In other words, we will make it so that UÌ‚âº is a linear transformation of the
temporary variable, rather than an affine transformation, absorbing u into the
temporary variable.
So, consider a lower triangular sÃ—s matrix Î² and an NÃ—s matrix V such that
    UÌ‚âº = V * Î²áµ€.
This allows us to rewrite the matrix equations as
    V * Î²áµ€ = u âŠ— ğŸ™áµ€ + Î”t * F * aÌ‚áµ€ and
    F = FÌ‚ + Î”t * J * F * Î³áµ€ + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
Solving the first equation for F tells us that
    F = Î”tâ»Â¹ * V * (aÌ‚â»Â¹ * Î²)áµ€ - Î”tâ»Â¹ * u âŠ— (aÌ‚â»Â¹ * ğŸ™)áµ€.
Substituting this into the second equation gives us
    Î”tâ»Â¹ * V * (aÌ‚â»Â¹ * Î²)áµ€ - Î”tâ»Â¹ * u âŠ— (aÌ‚â»Â¹ * ğŸ™)áµ€ =
        = FÌ‚ + J * V * (Î³ * aÌ‚â»Â¹ * Î²)áµ€ - J * u âŠ— (Î³ * aÌ‚â»Â¹ * ğŸ™)áµ€ +
          Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
Multiplying through on the right by Î”t * (Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ and rearranging
results in
    V * (Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²)áµ€ - Î”t * J * V =
        = u âŠ— (Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™)áµ€ - Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ +
          Î”t * FÌ‚ * (Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ + Î”tÂ² * fÌ‡ âŠ— (Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€.
Since Î³, aÌ‚, and Î² are all lower triangular, Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î² is also
lower triangular, which means that
    Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î² =
        diag(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²) + lower(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²).
In addition, we have that
    diag(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²) =
        = diag(Î²â»Â¹) * diag(aÌ‚) * diag(Î³â»Â¹) * diag(aÌ‚â»Â¹) * diag(Î²) =
        = diag(Î²)â»Â¹ * diag(aÌ‚) * diag(Î³)â»Â¹ * diag(aÌ‚)â»Â¹ * diag(Î²) =
        = diag(Î³)â»Â¹.
Combining the last two expressions gives us
    Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î² = diag(Î³)â»Â¹ + lower(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²).
Substituting this into the last equation for V gives us
    V * (diag(Î³)â»Â¹)áµ€ + V * lower(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²)áµ€ - Î”t * J * V =
        = u âŠ— (Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™)áµ€ - Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ +
          Î”t * FÌ‚ * (Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ + Î”tÂ² * fÌ‡ âŠ— (Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€.
Multiplying through on the right by diag(Î³)áµ€ and rearranging tells us that
    V - Î”t * J * V * diag(Î³)áµ€ =
        = u âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™)áµ€ -
          Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ * diag(Î³)áµ€ +
          Î”t * FÌ‚ * (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ +
          Î”tÂ² * fÌ‡ âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€ -
          V * (diag(Î³) * lower(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²))áµ€.
Since lower() preserves multiplication by a diagonal matrix, we can rewrite
this as
    V - Î”t * J * V * diag(Î³)áµ€ =
        = u âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™)áµ€ -
          Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ * diag(Î³)áµ€ +
          Î”t * FÌ‚ * (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ +
          Î”tÂ² * fÌ‡ âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€ -
          V * lower(diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²)áµ€.

We will now show that this reformulation will not allow us to eliminate
multiplications by J, as the previous ones did.
If we wanted to factor out all multiplications by J when we convert back out of
matrix form, we would rearrange the last equation to get
    (V - u âŠ— (Î²â»Â¹ * ğŸ™)áµ€) - Î”t * J * (V - u âŠ— (Î²â»Â¹ * ğŸ™)áµ€) * diag(Î³)áµ€ =
        = u âŠ— ((diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ - Î²â»Â¹) * ğŸ™)áµ€ +
          Î”t * FÌ‚ * (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ +
          Î”tÂ² * fÌ‡ âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€ -
          V * lower(diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²)áµ€.
In order to apply limiters on an unscaled state, we would then require that the
coefficient of u on the right-hand side of this equation be ğŸ™áµ€; i.e., that
    (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ - Î²â»Â¹) * ğŸ™ = ğŸ™.
In general, this equation does not have a solution Î²; e.g., if Î³ = aÌ‚ = d * I for
some scalar constant d, then it simplifies to
    (Î²â»Â¹ - Î²â»Â¹) * ğŸ™ = ğŸ™.
Even if we were to use more complex transformations, it would still be
impossible to eliminate multiplications by J while preserving an unscaled state
on the right-hand side.
For example, if we were to instead set UÌ‚âº = u âŠ— Î´áµ€ + V * Î²áµ€ for some vector Î´ of
length s, the above equation would become
    (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ - Î²â»Â¹) * (Î´ - ğŸ™) = ğŸ™.
This also does not have a general solution.

So, we will proceed without rearranging the last equation for V, and we will
require that the coefficient of u in it be ğŸ™áµ€; i.e., that
    diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™ = ğŸ™.
This equation has infinitely many solutions; the easiest way to obtain a
solution is to set
    diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ = I.
This implies that
    Î²â»Â¹ = diag(Î³)â»Â¹ * aÌ‚ * Î³ * aÌ‚â»Â¹ and
    Î² = aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * diag(Î³).
Substituting this into the last equation for V gives us
    V - Î”t * J * V * diag(Î³)áµ€ =
        = u âŠ— ğŸ™áµ€ - Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ * diag(Î³)áµ€ + Î”t * FÌ‚ * aÌ‚áµ€ +
          Î”tÂ² * fÌ‡ âŠ— (aÌ‚ * Î³ * ğŸ™)áµ€ - V * lower(Î²)áµ€.
Taking this and the equation UÌ‚âº = V * Î²áµ€ back out of matrix form gives us
    UÌ‚âºáµ¢ = âˆ‘_{j=1}^i Î²áµ¢â±¼ * Váµ¢ and
    Váµ¢ - Î”t * Î³áµ¢áµ¢ * J * Váµ¢ =
        = u - Î”t * Î³áµ¢áµ¢ * J * u * âˆ‘_{j=1}^i (Î²â»Â¹)áµ¢â±¼ +
          Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * f(UÌ‚â±¼, TÌ‚â±¼) + Î”tÂ² * fÌ‡ * âˆ‘_{j=1}^i (aÌ‚ * Î³)áµ¢â±¼ -
          âˆ‘_{j=1}^{i-1} Î²áµ¢â±¼ * Vâ±¼.
Thus, to enable the use of limiters, we can redefine
    UÌ‚âºáµ¢ := âˆ‘_{j=1}^i Î²áµ¢â±¼ * Váµ¢, where
    Váµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * (
              u - Î”t * Î³áµ¢áµ¢ * J * u * âˆ‘_{j=1}^i (Î²â»Â¹)áµ¢â±¼ +
              Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * f(UÌ‚â±¼, TÌ‚â±¼) + Î”tÂ² * fÌ‡ * âˆ‘_{j=1}^i (aÌ‚ * Î³)áµ¢â±¼ -
              âˆ‘_{j=1}^{i-1} Î²áµ¢â±¼ * Vâ±¼
          ).

To actually use a limiter, we must replace the use of f(u, t) in the equation
for Váµ¢ with a new function g(uâ‚Š, u, t, Î”t), where
    g(uâ‚Š, u, t, Î”t) := uâ‚Š + Î”t * f(u, t).
Internally, g can use a different tendency function fÌƒ(u, t), and it can apply a
limiter L after incrementing a state with the output of fÌƒ, so that
    g(uâ‚Š, u, t, Î”t) = L(uâ‚Š + Î”t * fÌƒ(u, t)).
Rewriting the last definition of Váµ¢ in terms of g instead of f gives us
    Váµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * g(
              u - Î”t * Î³áµ¢áµ¢ * J * u * âˆ‘_{j=1}^i (Î²â»Â¹)áµ¢â±¼ +
              Î”t * âˆ‘_{j=1}^{i-1} aÌ‚áµ¢â±¼ * f(UÌ‚â±¼, TÌ‚â±¼) +
              Î”tÂ² * fÌ‡ * âˆ‘_{j=1}^i (aÌ‚ * Î³)áµ¢â±¼ - âˆ‘_{j=1}^{i-1} Î²áµ¢â±¼ * Vâ±¼,
              UÌ‚áµ¢,
              TÌ‚áµ¢
              Î”t * aÌ‚áµ¢áµ¢,
          ).
=#

struct RosenbrockAlgorithm{Î³, a, b, L, M}
    linsolve:::L
    multiply::M
end
RosenbrockAlgorithm{Î³, a, b}(
    linsolve::L,
    multiply::M = nothing,
) where {Î³, a, b, L, M} = RosenbrockAlgorithm{Î³, a, b, L, M}(linsolve, multiply)

function check_valid_parameters(::RosenbrockAlgorithm{Î³, a, b}) where {Î³, a, b}
    
end

struct RosenbrockCache{C, L, M}
    _cache::C
    linsolve!::L
    multiply!::M
end

# TODO: Minimize allocations by reusing temporary variables after they are no
# longer needed.
function cache(
    prob::DiffEqBase.AbstractODEProblem,
    alg::RosenbrockAlgorithm;
    kwargs...
)
    check_valid_parameters(alg)
    s = length(b)
    increment_mode = prob.f isa ForwardEulerODEFunction
    u_prototype = prob.u0
    j_prototype = prob.f.jac_prototype
    linsolve! = alg.linsolve(Val{:init}, j_prototype, u_prototype)
    if isnothing(alg.multiply)
        if increment_mode
            error("RosenbrockAlgorithm.multiply must be specified when using a \
                   ForwardEulerODEFunction")
        end
        multiply! = nothing
    else
        multiply! = alg.multiply(Val{:init}, j_prototype, u_prototype)
    end
    _cache = NamedTuple((
        :UÌ‚ => similar(u_prototype),
        (increment_mode ? (FÌ‚s => map(i -> similar(u_prototype), 1:s),) : ())...,
        (increment_mode ? :Vs : :Ks) => map(i -> similar(u_prototype), 1:s),
    ))
    C = typeof(_cache)
    L = typeof(linsolve!)
    M = typeof(multiply!)
    return RosenbrockCache{C, L, M}(_cache, linsolve!, multiply!)
end

step_u!(integrator, cache::RosenbrockCache) =
    rosenbrock_step_u!(integrator, cache, integrator.prob.f)

function precomputed_values(
    ::RosenbrockAlgorithm{Î³, a, b},
    ::ForwardEulerODEFunction
) where {Î³, a, b}

end

function rosenbrock_step_u!(integrator, cache, f::ForwardEulerODEFunction)
    (; u, p, t, dt, alg) = integrator
    (; linsolve!, multiply!) = cache
    (; UÌ‚, FÌ‚s, Vs) = cache._cache
end
