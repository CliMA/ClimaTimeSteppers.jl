#=
## Introduction to Rosenbrock Methods

An s-stage diagonally implicit Runge-Kutta (DIRK) method for solving
âˆ‚u/âˆ‚t = f(u, t) is specified by a lower triangular sÃ—s matrix a (whose values
are called "internal coefficients"), a vector b of length s (whose values are
called "weights"), and a vector c of length s (whose values are called
"abcissae" or "nodes").
Given the state u at time t, the state u_next at time t + Î”t is given by
    u_next := u + Î”t * âˆ‘_{i=1}^s báµ¢ * f(Uáµ¢, t + Î”t * cáµ¢), where
    Uáµ¢ := u + Î”t * âˆ‘_{j=1}^i aáµ¢â±¼ * f(Uâ±¼, t + Î”t * câ±¼).
In order to transform this DIRK method into a Rosenbrock method, we must assume
that it is "internally consistent", which means that
    cáµ¢ := âˆ‘_{j=1}^i aáµ¢â±¼.

First, we will simplify our notation by defining
    Táµ¢ := t + Î”t * âˆ‘_{j=1}^i aáµ¢â±¼ and
    Fáµ¢ := f(Uáµ¢, Táµ¢).
This simplifies our previous expressions to
    u_next = u + Î”t * âˆ‘_{i=1}^s báµ¢ * Fáµ¢ and
    Uáµ¢ = u + Î”t * âˆ‘_{j=1}^i aáµ¢â±¼ * Fâ±¼.
Next, we will define
    UÌ‚áµ¢ := u + Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼ * Fâ±¼ and
    TÌ‚áµ¢ := t + Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼.
This implies that
    Uáµ¢ = UÌ‚áµ¢ + Î”t * aáµ¢áµ¢ * Fáµ¢ and
    Táµ¢ = TÌ‚áµ¢ + Î”t * aáµ¢áµ¢.
Substituting this into the definition of Fáµ¢ gives us
    Fáµ¢ = f(UÌ‚áµ¢ + Î”t * aáµ¢áµ¢ * Fáµ¢, TÌ‚áµ¢ + Î”t * aáµ¢áµ¢).
Approximating the value of f with a first-order Taylor series expansion around
UÌ‚áµ¢ and TÌ‚áµ¢ gives us
    Fáµ¢ â‰ˆ f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J(UÌ‚áµ¢, TÌ‚áµ¢) * aáµ¢áµ¢ * Fáµ¢ + Î”t * fÌ‡(UÌ‚áµ¢, TÌ‚áµ¢) * aáµ¢áµ¢, where
    J(u, t) := âˆ‚f/âˆ‚u(u, t) and
    fÌ‡(u, t) := âˆ‚f/âˆ‚t(u, t).
This is roughly equivalent to running a single Newton iteration to solve for Fáµ¢,
starting with an initial guess of f(UÌ‚áµ¢, TÌ‚áµ¢) (or, equivalently, to solve for Uáµ¢,
starting with an initial guess of UÌ‚áµ¢).

In order to obtain a Rosenbrock method, we must modify this equation for Fáµ¢.
By using only a single Newton iteration, we break the assumptions that provide
guarantees about the convergence of the DIRK method.
To remedy this, we introduce an additional lower triangular sÃ—s matrix of
coefficients Î³, and we redefine Fáµ¢ to be the solution to
    Fáµ¢ = f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J(UÌ‚áµ¢, TÌ‚áµ¢) * âˆ‘_{j=1}^i Î³áµ¢â±¼ * Fâ±¼ +
         Î”t * fÌ‡(UÌ‚áµ¢, TÌ‚áµ¢) * âˆ‘_{j=1}^i Î³áµ¢â±¼.
In other words, we account for the higher-order terms dropped by the first-order
Taylor series expansion by taking carefully chosen linear combinations of Fâ±¼.
Since this effectively replaces aáµ¢áµ¢ with entries in Î³, the diagonal entries can
now be omitted from a, making it strictly lower triangular.

For a "modified Rosenbrock method", the coefficients are chosen so that
J(UÌ‚áµ¢, TÌ‚áµ¢) and fÌ‡(UÌ‚áµ¢, TÌ‚áµ¢) can be approximated by their values at time t, J(u, t)
and fÌ‡(u, t).
For a "W-method", the approximation can be even coarser; e.g., the values at a
previous timestep, or the values at a precomputed reference state, or even
approximations that do not correspond to any particular state.
So, we will modify the last equation by replacing J(UÌ‚áµ¢, TÌ‚áµ¢) and fÌ‡(UÌ‚áµ¢, TÌ‚áµ¢) with
some approximations J and fÌ‡ (whose values must satisfy the constraints of the
specific Rosenbrock method being used), giving us
    Fáµ¢ = f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J * âˆ‘_{j=1}^i Î³áµ¢â±¼ * Fâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼.
Solving this equation for Fáµ¢ lets us redefine
    Fáµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ +
              Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ).
Since multiplying âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ by J is more computationally expensive
than subtracting it from another value, we will rewrite this as
    Fáµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) + Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ +
              Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ) - Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼.

So, in summary, a Rosenbrock method is defined by some lower triangular sÃ—s
matrix Î³, some strictly lower triangular sÃ—s matrix a, some vector b of length
s, and some approximations J and fÌ‡ of âˆ‚f/âˆ‚u(UÌ‚áµ¢, TÌ‚áµ¢) and âˆ‚f/âˆ‚t(UÌ‚áµ¢, TÌ‚áµ¢), which
are all used to compute
    u_next := u + Î”t * âˆ‘_{i=1}^s báµ¢ * Fáµ¢, where
    UÌ‚áµ¢ := u + Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼ * Fâ±¼,
    TÌ‚áµ¢ := t + Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼, and
    Fáµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) + Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ +
              Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ) - Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼.

## Converting to Matrix Form

To simplify our further reformulations, we will convert our definitions to
matrix form.

First, we will reduce the number of matrix equations we need to analyze by
defining
    aÌ‚áµ¢â±¼ := i < s ? aâ‚áµ¢â‚Šâ‚â‚â±¼ : bâ±¼ and
    UÌ‚âºáµ¢ := i < s ? UÌ‚áµ¢â‚Šâ‚ : u_next.
We can then redefine
    u_next := UÌ‚âºâ‚› and
    UÌ‚áµ¢ := i == 1 ? u : UÌ‚âºáµ¢â‚‹â‚, where
    UÌ‚âºáµ¢ := u + Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * Fâ±¼.

The equations we will convert to matrix form are
    UÌ‚âºáµ¢ = u + Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * Fâ±¼ and
    Fáµ¢ = f(UÌ‚áµ¢, TÌ‚áµ¢) + Î”t * J * âˆ‘_{j=1}^i Î³áµ¢â±¼ * Fâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼.
Rewriting these with an explicit element index n âˆˆ 1:N (where N = length(u))
gives us
    UÌ‚âºáµ¢[n] = u[n] + Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * Fâ±¼[n] and
    Fáµ¢[n] = f(UÌ‚áµ¢, TÌ‚áµ¢)[n] + Î”t * âˆ‘_{m=1}^N J[n,m] * âˆ‘_{j=1}^i Î³áµ¢â±¼ * Fâ±¼[m] +
            Î”t * fÌ‡[n] * âˆ‘_{j=1}^i Î³áµ¢â±¼.
Now, we will formally define the vectors and matrices
    ğŸ™ âˆˆ â„Ë¢     | ğŸ™áµ¢ := 1,
    u âˆˆ â„á´º     | uâ‚™ := u[n],
    fÌ‡ âˆˆ â„á´º     | fÌ‡â‚™ := fÌ‡[n], and
    UÌ‚âº âˆˆ â„á´ºÃ—â„Ë¢ | UÌ‚âºâ‚™áµ¢ := UÌ‚âºáµ¢[n],
    F âˆˆ â„á´ºÃ—â„Ë¢  | Fâ‚™áµ¢ := Fáµ¢[n],
    FÌ‚ âˆˆ â„á´ºÃ—â„Ë¢  | FÌ‚â‚™áµ¢ := f(UÌ‚áµ¢, TÌ‚áµ¢)[n],
    J âˆˆ â„á´ºÃ—â„á´º  | Jâ‚™â‚˜ := J[n,m].
(If J and fÌ‡ are different for each stage, they may be replaced with tensors in
all of the following manipulations.)
We then have that
    UÌ‚âºâ‚™áµ¢ = uâ‚™ * ğŸ™áµ¢ + Î”t * âˆ‘_{j=1}^i Fâ‚™â±¼ * aÌ‚áµ¢â±¼ and
    Fâ‚™áµ¢ = FÌ‚â‚™áµ¢ + Î”t * âˆ‘_{m=1}^N Jâ‚™â‚˜ * âˆ‘_{j=1}^i Fâ‚˜â±¼ * Î³áµ¢â±¼ +
          Î”t * fÌ‡â‚™ * âˆ‘_{j=1}^i ğŸ™â±¼ * Î³áµ¢â±¼.
We can rewrite this as
    UÌ‚âºâ‚™áµ¢ = uâ‚™ * (ğŸ™áµ€)áµ¢ + Î”t * âˆ‘_{j=1}^s Fâ‚™â±¼ * (aÌ‚áµ€)â±¼áµ¢ and
    Fâ‚™áµ¢ = FÌ‚â‚™áµ¢ + Î”t * âˆ‘_{m=1}^N Jâ‚™â‚˜ * âˆ‘_{j=1}^s Fâ‚˜â±¼ * (Î³áµ€)â±¼áµ¢ +
          Î”t * fÌ‡â‚™ * âˆ‘_{j=1}^s (ğŸ™áµ€)â±¼ * (Î³áµ€)â±¼áµ¢.
Combining matrix-matrix and matrix-vector products gives us
    UÌ‚âºâ‚™áµ¢ = uâ‚™ * (ğŸ™áµ€)áµ¢ + Î”t * (F * aÌ‚áµ€)â‚™áµ¢ and
    Fâ‚™áµ¢ = FÌ‚â‚™áµ¢ + Î”t * (J * F * Î³áµ€)â‚™áµ¢ + Î”t * fÌ‡â‚™ * (Î³ * ğŸ™)áµ¢.
Dropping the indices then tells us that
    UÌ‚âº = u âŠ— ğŸ™áµ€ + Î”t * F * aÌ‚áµ€ and
    F = FÌ‚ + Î”t * J * F * Î³áµ€ + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
To rewrite this in a way that more directly corresponds to our last formulation,
we will define the functions diag() and lower(), such that, for any lower
triangular matrix M,
    M = diag(M) + lower(M), where
    diag(M)áµ¢â±¼ := i == j ? Máµ¢â±¼ : 0 and
    lower(M)áµ¢â±¼ := i > j ? Máµ¢â±¼ : 0.
This lets us rewrite the equation for F as
    F * (I + diag(Î³)â»Â¹ * lower(Î³))áµ€ -
    Î”t * J * F * (I + diag(Î³)â»Â¹ * lower(Î³))áµ€ * diag(Î³)áµ€ =
        = FÌ‚ + F * (diag(Î³)â»Â¹ * lower(Î³))áµ€ + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.

We will now use these matrix equations to define two reformulations: one that
optimizes performance by eliminating the subtraction after the linear solve, and
one that enables limiters by appropriately modifying the value being subtracted.

## Optimizing Performance

Let us define a new NÃ—s matrix
    K := F * Î³áµ€.
We then have that
    F = K * (Î³â»Â¹)áµ€.
This allows us to rewrite the matrix equations as
    UÌ‚âº = u âŠ— ğŸ™áµ€ + Î”t * K * (aÌ‚ * Î³â»Â¹)áµ€ and
    K * (Î³â»Â¹)áµ€ = FÌ‚ + Î”t * J * K + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
Since Î³ is lower triangular,
    Î³â»Â¹ = diag(Î³â»Â¹) + lower(Î³â»Â¹) = diag(Î³)â»Â¹ + lower(Î³â»Â¹).
This lets us rewrite the equation for K as
    K * (diag(Î³)â»Â¹ + lower(Î³â»Â¹))áµ€ = FÌ‚ + Î”t * J * K + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
Multiplying through on the right by diag(Î³)áµ€ and rearranging gives us
    K - Î”t * J * K * diag(Î³)áµ€ =
        (FÌ‚ - K * lower(Î³â»Â¹)áµ€ + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€) * diag(Î³)áµ€.
Taking this and the equation for UÌ‚âº back out of matrix form gives us
    UÌ‚âºáµ¢ = u + Î”t * âˆ‘_{j=1}^i (aÌ‚ * Î³â»Â¹)áµ¢â±¼ * Kâ±¼ and
    Káµ¢ - Î”t * Î³áµ¢áµ¢ * J * Káµ¢ =
        = Î³áµ¢áµ¢ *
          (f(UÌ‚áµ¢, TÌ‚áµ¢) - âˆ‘_{j=1}^{i-1} (Î³â»Â¹)áµ¢â±¼ * Kâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼).
Thus, to optimize performance, we can redefine
    UÌ‚âºáµ¢ := u + Î”t * âˆ‘_{j=1}^i (aÌ‚ * Î³â»Â¹)áµ¢â±¼ * Kâ±¼, where
    Káµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * Î³áµ¢áµ¢ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) - âˆ‘_{j=1}^{i-1} (Î³â»Â¹)áµ¢â±¼ * Kâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ).

## Enabling Limiters

In the previous section, we changed the temporary variable from Fáµ¢, which is an
approximation of f(Uáµ¢, Táµ¢), to Káµ¢, which is a linear combination of previously
computed values of Fáµ¢.
Now, we will change the temporary variable to Váµ¢, so that the approximations of
Uáµ¢ and u_next will be linear combinations of previously computed values of Váµ¢.
In other words, we will make it so that UÌ‚âº is a linear transformation of the
temporary variable, rather than an affine transformation, absorbing u into the
temporary variable.
So, consider a lower triangular sÃ—s matrix Î² and an NÃ—s matrix V such that
    UÌ‚âº = V * Î²áµ€.
We can then rewrite the matrix equations as
    V * Î²áµ€ = u âŠ— ğŸ™áµ€ + Î”t * F * aÌ‚áµ€ and
    F = FÌ‚ + Î”t * J * F * Î³áµ€ + Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
Solving the first equation for F tells us that
    F = Î”tâ»Â¹ * V * (aÌ‚â»Â¹ * Î²)áµ€ - Î”tâ»Â¹ * u âŠ— (aÌ‚â»Â¹ * ğŸ™)áµ€.
Substituting this into the second equation gives us
    Î”tâ»Â¹ * V * (aÌ‚â»Â¹ * Î²)áµ€ - Î”tâ»Â¹ * u âŠ— (aÌ‚â»Â¹ * ğŸ™)áµ€ =
        = FÌ‚ + J * V * (Î³ * aÌ‚â»Â¹ * Î²)áµ€ - J * u âŠ— (Î³ * aÌ‚â»Â¹ * ğŸ™)áµ€ +
          Î”t * fÌ‡ âŠ— (Î³ * ğŸ™)áµ€.
Multiplying through on the right by Î”t * (Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ and rearranging
results in
    V * (Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²)áµ€ - Î”t * J * V =
        = u âŠ— (Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™)áµ€ - Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ +
          Î”t * FÌ‚ * (Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ + Î”tÂ² * fÌ‡ âŠ— (Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€.
Since Î³, aÌ‚, and Î² are all lower triangular, Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î² is also
lower triangular, which means that
    Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î² =
        diag(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²) + lower(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²).
In addition, we have that
    diag(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²) =
        = diag(Î²â»Â¹) * diag(aÌ‚) * diag(Î³â»Â¹) * diag(aÌ‚â»Â¹) * diag(Î²) =
        = diag(Î²)â»Â¹ * diag(aÌ‚) * diag(Î³)â»Â¹ * diag(aÌ‚)â»Â¹ * diag(Î²) =
        = diag(Î³)â»Â¹.
Combining the last two expressions gives us
    Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î² = diag(Î³)â»Â¹ + lower(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²).
Substituting this into the last equation for V gives us
    V * (diag(Î³)â»Â¹)áµ€ + V * lower(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²)áµ€ - Î”t * J * V =
        = u âŠ— (Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™)áµ€ - Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ +
          Î”t * FÌ‚ * (Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ + Î”tÂ² * fÌ‡ âŠ— (Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€.
Multiplying through on the right by diag(Î³)áµ€ and rearranging tells us that
    V - Î”t * J * V * diag(Î³)áµ€ =
        = u âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™)áµ€ -
          Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ * diag(Î³)áµ€ +
          Î”t * FÌ‚ * (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ +
          Î”tÂ² * fÌ‡ âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€ -
          V * (diag(Î³) * lower(Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²))áµ€.
Since lower() preserves multiplication by a diagonal matrix, we can rewrite
this as
    V - Î”t * J * V * diag(Î³)áµ€ =
        = u âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™)áµ€ -
          Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ * diag(Î³)áµ€ +
          Î”t * FÌ‚ * (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ +
          Î”tÂ² * fÌ‡ âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€ -
          V * lower(diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²)áµ€.

We will now show that this reformulation will not allow us to eliminate
multiplications by J, as the previous ones did.
If we wanted to factor out all multiplications by J when we convert back out of
matrix form, we would rearrange the last equation to get
    (V - u âŠ— (Î²â»Â¹ * ğŸ™)áµ€) - Î”t * J * (V - u âŠ— (Î²â»Â¹ * ğŸ™)áµ€) * diag(Î³)áµ€ =
        = u âŠ— ((diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ - Î²â»Â¹) * ğŸ™)áµ€ +
          Î”t * FÌ‚ * (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹)áµ€ +
          Î”tÂ² * fÌ‡ âŠ— (diag(Î³) * Î²â»Â¹ * aÌ‚ * ğŸ™)áµ€ -
          V * lower(diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * Î²)áµ€.
In order to apply limiters on an unscaled state, we would then require that the
coefficient of u on the right-hand side of this equation be ğŸ™áµ€; i.e., that
    (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ - Î²â»Â¹) * ğŸ™ = ğŸ™.
In general, this equation does not have a solution Î²; e.g., if Î³ = aÌ‚ = d * I for
some scalar constant d, then the equation simplifies to
    (Î²â»Â¹ - Î²â»Â¹) * ğŸ™ = ğŸ™.
Even if we were to use more complex transformations, it would still be
impossible to eliminate multiplications by J while preserving an unscaled state
on the right-hand side.
For example, if we were to instead set UÌ‚âº = u âŠ— Î´áµ€ + V * Î²áµ€ for some vector Î´ of
length s, the above equation would become
    (diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ - Î²â»Â¹) * (Î´ - ğŸ™) = ğŸ™.
This also does not have a general solution.

So, we must proceed without rearranging the last equation for V.
In order to apply limiters on an unscaled state, we must require that the
coefficient of u in that equation be ğŸ™áµ€, which implies that
    diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™ = ğŸ™.

We will now show that we cannot also make the coefficient of the J term on the
right-hand side be the same as the one on the left-hand side.
If we wanted this to be the case, we would also have to satisfy the equation
    Î²â»Â¹ * ğŸ™ = ğŸ™.
In general, we cannot simultaneously satisfy both of the last two equations;
e.g., if Î³ = d * I for some scalar constant d, we can rearrange the equations to
get that
    Î² * ğŸ™ = d * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™ and Î² * ğŸ™ = ğŸ™.
Unless d * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™ = ğŸ™ (which will not be the case in general), this
system of equations cannot be satisfied.

So, we will only require that Î² satisfies the equation
    diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * ğŸ™ = ğŸ™.
This equation has infinitely many solutions; the easiest way to obtain a
solution is to set
    diag(Î³) * Î²â»Â¹ * aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ = I.
This implies that
    Î²â»Â¹ = diag(Î³)â»Â¹ * aÌ‚ * Î³ * aÌ‚â»Â¹ and
    Î² = aÌ‚ * Î³â»Â¹ * aÌ‚â»Â¹ * diag(Î³).
Substituting this into the last equation for V gives us
    V - Î”t * J * V * diag(Î³)áµ€ =
        = u âŠ— ğŸ™áµ€ - Î”t * J * u âŠ— (Î²â»Â¹ * ğŸ™)áµ€ * diag(Î³)áµ€ + Î”t * FÌ‚ * aÌ‚áµ€ +
          Î”tÂ² * fÌ‡ âŠ— (aÌ‚ * Î³ * ğŸ™)áµ€ - V * lower(Î²)áµ€.
Taking this and the equation UÌ‚âº = V * Î²áµ€ back out of matrix form gives us
    UÌ‚âºáµ¢ = âˆ‘_{j=1}^i Î²áµ¢â±¼ * Váµ¢ and
    Váµ¢ - Î”t * Î³áµ¢áµ¢ * J * Váµ¢ =
        = u - Î”t * Î³áµ¢áµ¢ * J * u * âˆ‘_{j=1}^i (Î²â»Â¹)áµ¢â±¼ +
          Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * f(UÌ‚â±¼, TÌ‚â±¼) + Î”tÂ² * fÌ‡ * âˆ‘_{j=1}^i (aÌ‚ * Î³)áµ¢â±¼ -
          âˆ‘_{j=1}^{i-1} Î²áµ¢â±¼ * Vâ±¼.
Thus, to enable the use of limiters, we can redefine
    UÌ‚âºáµ¢ := âˆ‘_{j=1}^i Î²áµ¢â±¼ * Váµ¢, where
    Váµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * (
              u - Î”t * Î³áµ¢áµ¢ * J * u * âˆ‘_{j=1}^i (Î²â»Â¹)áµ¢â±¼ +
              Î”t * âˆ‘_{j=1}^i aÌ‚áµ¢â±¼ * f(UÌ‚â±¼, TÌ‚â±¼) + Î”tÂ² * fÌ‡ * âˆ‘_{j=1}^i (aÌ‚ * Î³)áµ¢â±¼ -
              âˆ‘_{j=1}^{i-1} Î²áµ¢â±¼ * Vâ±¼
          ).

To actually use a limiter, we must replace the use of f(u, t) in the equation
for Váµ¢ with a new function g(uâ‚Š, u, t, Î”t), where
    g(uâ‚Š, u, t, Î”t) := uâ‚Š + Î”t * f(u, t).
Internally, g can use a different tendency function fÌƒ(u, t), and it can apply a
limiter L after incrementing a state with the output of fÌƒ, so that
    g(uâ‚Š, u, t, Î”t) = L(uâ‚Š + Î”t * fÌƒ(u, t)).
Rewriting the last definition of Váµ¢ in terms of g instead of f gives us
    Váµ¢ := (I - Î”t * Î³áµ¢áµ¢ * J)â»Â¹ * g(
              u - Î”t * Î³áµ¢áµ¢ * J * u * âˆ‘_{j=1}^i (Î²â»Â¹)áµ¢â±¼ +
              Î”t * âˆ‘_{j=1}^{i-1} aÌ‚áµ¢â±¼ * f(UÌ‚â±¼, TÌ‚â±¼) +
              Î”tÂ² * fÌ‡ * âˆ‘_{j=1}^i (aÌ‚ * Î³)áµ¢â±¼ - âˆ‘_{j=1}^{i-1} Î²áµ¢â±¼ * Vâ±¼,
              UÌ‚áµ¢,
              TÌ‚áµ¢
              Î”t * aÌ‚áµ¢áµ¢,
          ).

## Negating Wfact

For some reason, OrdinaryDiffEq defines Wfact as Î”t * Î³áµ¢áµ¢ * J - I, so we must
negate all of our temporary variables.

For the original formulation, this means that
    UÌ‚áµ¢ := u - Î”t * âˆ‘_{j=1}^{i-1} aáµ¢â±¼ * Fâ±¼, where
    Fáµ¢ := (Î”t * Î³áµ¢áµ¢ * J - I)â»Â¹ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) - Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼ +
              Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ) + Î³áµ¢áµ¢â»Â¹ * âˆ‘_{j=1}^{i-1} Î³áµ¢â±¼ * Fâ±¼.
For the performance formulation, this means that
    UÌ‚âºáµ¢ := u - Î”t * âˆ‘_{j=1}^i (aÌ‚ * Î³â»Â¹)áµ¢â±¼ * Kâ±¼, where
    Káµ¢ := (Î”t * Î³áµ¢áµ¢ * J - I)â»Â¹ * Î³áµ¢áµ¢ * (
              f(UÌ‚áµ¢, TÌ‚áµ¢) + âˆ‘_{j=1}^{i-1} (Î³â»Â¹)áµ¢â±¼ * Kâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼
          ).
For the limiters formulation, this means that
    UÌ‚âºáµ¢ := -âˆ‘_{j=1}^i Î²áµ¢â±¼ * Váµ¢, where
    Váµ¢ := (Î”t * Î³áµ¢áµ¢ * J - I)â»Â¹ * g(
              u - Î”t * Î³áµ¢áµ¢ * J * u * âˆ‘_{j=1}^i (Î²â»Â¹)áµ¢â±¼ +
              Î”t * âˆ‘_{j=1}^{i-1} aÌ‚áµ¢â±¼ * f(UÌ‚â±¼, TÌ‚â±¼) +
              Î”tÂ² * fÌ‡ * âˆ‘_{j=1}^i (aÌ‚ * Î³)áµ¢â±¼ + âˆ‘_{j=1}^{i-1} Î²áµ¢â±¼ * Vâ±¼,
              UÌ‚áµ¢,
              TÌ‚áµ¢
              Î”t * aÌ‚áµ¢áµ¢,
          ).
=#
import LinearAlgebra
import StaticArrays: SUnitRange, SOneTo
import Base: broadcasted, materialize!

struct RosenbrockAlgorithm{Î³, a, b, U, L, M, S} <: DistributedODEAlgorithm
    update_jac::U
    linsolve::L
    multiply!::M
    set_Î”tÎ³!::S
end
RosenbrockAlgorithm{Î³, a, b}(;
    update_jac::U = UpdateEvery(NewStep()),
    linsolve::L,
    multiply!::M = nothing,
    set_Î”tÎ³!::S = nothing,
) where {Î³, a, b, U, L, M, S} =
    RosenbrockAlgorithm{Î³, a, b, U, L, M, S}(
        update_jac,
        linsolve,
        multiply!,
        set_Î”tÎ³!,
    )

@generated foreachval(f::F, ::Val{N}) where {F, N} =
    quote
        Base.@nexprs $N i -> f(Val(i))
        return nothing
    end
triangular_inv(matrix::T) where {T} =
    T(inv(LinearAlgebra.LowerTriangular(matrix)))
lower_plus_diag(matrix::T) where {T} = T(LinearAlgebra.LowerTriangular(matrix))
diag(matrix::T) where {T} = T(LinearAlgebra.Diagonal(matrix))
lower(matrix) = lower_plus_diag(matrix) - diag(matrix)
to_enumerated_rows(v::AbstractVector) = v
function to_enumerated_rows(m::AbstractMatrix)
    rows = tuple(1:size(m, 1)...)
    nonzero_indices = map(i -> findall(m[i, :] .!= 0), rows)
    enumerated_rows = map(
        i -> tuple(zip(nonzero_indices[i], m[i, nonzero_indices[i]])...),
        rows,
    )
    return enumerated_rows
end
linear_combination(enumerated_row, vectors) =
    map(((j, val),) -> broadcasted(*, val, vectors[j]), enumerated_row)
function scaled_linear_combination(enumerated_row, vectors, scale)
    unscaled_terms = linear_combination(enumerated_row, vectors)
    length(unscaled_terms) == 0 && return ()
    return (broadcasted(*, scale, broadcasted(+, unscaled_terms...)),)
end

num_stages(::Type{<:RosenbrockAlgorithm{Î³}}) where {Î³} = size(Î³, 1)

function check_valid_parameters(
    ::Type{<:RosenbrockAlgorithm{Î³, a, b, U}},
) where {Î³, a, b, U}
    Î³ === lower_plus_diag(Î³) ||
        error("Î³ must be a lower triangular matrix")
    a === lower(a) ||
        error("a must be a strictly lower triangular matrix")
    LinearAlgebra.det(Î³) != 0 ||
        error("non-invertible matrices Î³ are not currently supported")
    if U != UpdateEvery{NewStage}
        diag(Î³) === typeof(Î³)(Î³[1, 1] * I) ||
            error("Î³ must have a uniform diagonal when \
                   update_jac != UpdateEvery(NewStage())")
    end
    can_handle(U, NewStep()) || can_handle(U, NewStage()) ||
        error("update_jac must be able to handle NewStep() or NewStage()")
end
function check_valid_parameters(
    alg_type::Type{<:RosenbrockAlgorithm{Î³, a, b, U, L, M, S}},
    ::Type{<:ForwardEulerODEFunction},
) where {Î³, a, b, U, L, M, S}
    check_valid_parameters(alg_type)
    aÌ‚ = vcat(a[SUnitRange(2, length(b)), SOneTo(length(b))], transpose(b))
    LinearAlgebra.det(aÌ‚) != 0 ||
        error("non-invertible matrices aÌ‚ are not currently supported when \
               using ForwardEulerODEFunction")
    M != Nothing ||
        error("multiply! must be specified when using ForwardEulerODEFunction")
    S != Nothing ||
        error("set_Î”tÎ³! must be specified when using ForwardEulerODEFunction")
end
check_valid_parameters(alg_type, _) = check_valid_parameters(alg_type)

struct RosenbrockCache{C, U, L}
    _cache::C
    update_jac_cache::U
    linsolve!::L
end

# TODO: Minimize allocations by reusing temporary variables after they are no
# longer needed.
function cache(
    prob::DiffEqBase.AbstractODEProblem,
    alg::RosenbrockAlgorithm;
    kwargs...
)
    check_valid_parameters(typeof(alg), typeof(prob.f))

    s = num_stages(typeof(alg))
    u_prototype = prob.u0
    W_prototype = prob.f.jac_prototype
    increment_mode = prob.f isa ForwardEulerODEFunction

    _cache = NamedTuple((
        :UÌ‚âºáµ¢ => similar(u_prototype),
        (increment_mode ? (:Fs => map(i -> similar(u_prototype), 1:s),) : ())...,
        (increment_mode ? :Vs : :Ks) => map(i -> similar(u_prototype), 1:s),
        :W => similar(W_prototype),
        :fÌ‡ => similar(u_prototype),
    ))

    update_jac_cache = allocate_cache(alg.update_jac)

    linsolve! = alg.linsolve(Val{:init}, W_prototype, u_prototype)

    return RosenbrockCache(_cache, update_jac_cache, linsolve!)
end

step_u!(integrator, cache::RosenbrockCache) =
    rosenbrock_step_u!(integrator, cache, integrator.prob.f)

# The precomputed values are too complicated for constant propagation, so we use
# @generated to force the values to be compile-time constants.
@generated function precomputed_values(
    ::Type{<:RosenbrockAlgorithm{Î³, a, b}},
    _,
) where {Î³, a, b}
    aÌ‚ = vcat(a[2:end, :], transpose(b))
    Î³â»Â¹ = triangular_inv(Î³)
    lowerÎ³â»Â¹ = lower(Î³â»Â¹)
    aÌ‚Î³â»Â¹ = aÌ‚ * Î³â»Â¹
    diagÎ³ğŸ™ = vec(sum(diag(Î³), dims = 2))
    Î³ğŸ™ = vec(sum(Î³, dims = 2))
    aÌ‚ğŸ™ = vec(sum(aÌ‚, dims = 2))
    values = map(to_enumerated_rows, (; lowerÎ³â»Â¹, aÌ‚Î³â»Â¹, diagÎ³ğŸ™, Î³ğŸ™, aÌ‚ğŸ™))
    return :($values)
end
@generated function precomputed_values(
    ::Type{<:RosenbrockAlgorithm{Î³, a, b}},
    ::Type{<:ForwardEulerODEFunction},
) where {Î³, a, b}
    aÌ‚ = vcat(a[2:end, :], transpose(b))
    aÌ‚Î³ = aÌ‚ * Î³
    Î² = aÌ‚ * triangular_inv(aÌ‚Î³) * diag(Î³)
    loweraÌ‚ = lower(aÌ‚)
    lowerÎ² = lower(Î²)
    diagÎ³ğŸ™ = vec(sum(diag(Î³), dims = 2))
    aÌ‚ğŸ™ = vec(sum(aÌ‚, dims = 2))
    Î²â»Â¹ğŸ™ = vec(sum(triangular_inv(Î²), dims = 2))
    aÌ‚Î³ğŸ™ = vec(sum(aÌ‚Î³, dims = 2))
    diagaÌ‚ğŸ™ = vec(sum(diag(aÌ‚), dims = 2))
    values = map(
        to_enumerated_rows,
        (; loweraÌ‚, lowerÎ², Î², diagÎ³ğŸ™, aÌ‚ğŸ™, Î²â»Â¹ğŸ™, aÌ‚Î³ğŸ™, diagaÌ‚ğŸ™),
    )
    return :($values)
end

function rosenbrock_step_u!(integrator, cache, g::ForwardEulerODEFunction)
    (; u, p, t, dt, alg) = integrator
    (; update_jac, multiply!, set_Î”tÎ³!) = alg
    (; update_jac_cache, linsolve!) = cache
    (; UÌ‚âºáµ¢, Vs, Fs, W, fÌ‡) = cache._cache
    (; loweraÌ‚, lowerÎ², Î², diagÎ³ğŸ™, aÌ‚ğŸ™, Î²â»Â¹ğŸ™, aÌ‚Î³ğŸ™, diagaÌ‚ğŸ™) =
        precomputed_values(typeof(alg), typeof(g))
    function jac_func(UÌ‚áµ¢, TÌ‚áµ¢, Î³áµ¢áµ¢)
        g.Wfact(W, UÌ‚áµ¢, p, dt * Î³áµ¢áµ¢, TÌ‚áµ¢)
        !isnothing(g.tgrad) && g.tgrad(fÌ‡, UÌ‚áµ¢, p, TÌ‚áµ¢)
    end
    function stage_func(::Val{i}) where {i}
        Î³áµ¢áµ¢ = diagÎ³ğŸ™[i]
        UÌ‚áµ¢ = i == 1 ? u : UÌ‚âºáµ¢
        TÌ‚áµ¢ = i == 1 ? t : t + dt * aÌ‚ğŸ™[i]

        run!(update_jac, update_jac_cache, NewStage(), jac_func, UÌ‚áµ¢, TÌ‚áµ¢, Î³áµ¢áµ¢)

        # Váµ¢ = (Î”t * Î³áµ¢áµ¢ * J - I)â»Â¹ * g(
        #     u - Î”t * Î³áµ¢áµ¢ * J * u * âˆ‘_{j=1}^i (Î²â»Â¹)áµ¢â±¼ +
        #     Î”t * âˆ‘_{j=1}^{i-1} aÌ‚áµ¢â±¼ * f(UÌ‚â±¼, TÌ‚â±¼) +
        #     Î”tÂ² * fÌ‡ * âˆ‘_{j=1}^i (aÌ‚ * Î³)áµ¢â±¼ + âˆ‘_{j=1}^{i-1} Î²áµ¢â±¼ * Vâ±¼,
        #     UÌ‚áµ¢,
        #     TÌ‚áµ¢
        #     Î”t * aÌ‚áµ¢áµ¢,
        # )
        set_Î”tÎ³!(W, dt * Î³áµ¢áµ¢ * Î²â»Â¹ğŸ™[i], dt * Î³áµ¢áµ¢)
        multiply!(Vs[i], W, u)
        set_Î”tÎ³!(W, dt * Î³áµ¢áµ¢, dt * Î³áµ¢áµ¢ * Î²â»Â¹ğŸ™[i])
        Vs[i] .= broadcasted(
            +,
            broadcasted(-, Vs[i]),
            scaled_linear_combination(loweraÌ‚[i], Fs, dt)...,
            (isnothing(g.tgrad) ? () : (broadcasted(*, dt^2 * aÌ‚Î³ğŸ™[i], fÌ‡),))...,
            linear_combination(lowerÎ²[i], Vs)...,
        )
        Fs[i] .= Vs[i]
        g(Vs[i], UÌ‚áµ¢, p, TÌ‚áµ¢, dt * diagaÌ‚ğŸ™[i])
        Fs[i] .= (Fs[i] .- Vs[i]) ./ (dt * diagaÌ‚ğŸ™[i])
        linsolve!(Vs[i], W, Vs[i]) # assume that linsolve! can handle aliasing

        # UÌ‚âºáµ¢ = -âˆ‘_{j=1}^i Î²áµ¢â±¼ * Váµ¢
        UÌ‚âºáµ¢ .= scaled_linear_combination(Î²[i], Vs, -1)[1]
    end

    run!(update_jac, update_jac_cache, NewStep(), jac_func, u, t, diagÎ³ğŸ™[1])
    foreachval(stage_func, Val(num_stages(typeof(alg))))
    u .= UÌ‚âºáµ¢
end

function rosenbrock_step_u!(integrator, cache, f)
    (; u, p, t, dt, alg) = integrator
    (; update_jac) = alg
    (; update_jac_cache, linsolve!) = cache
    (; UÌ‚âºáµ¢, Ks, W, fÌ‡) = cache._cache
    (; lowerÎ³â»Â¹, aÌ‚Î³â»Â¹, diagÎ³ğŸ™, Î³ğŸ™, aÌ‚ğŸ™) =
        precomputed_values(typeof(alg), typeof(f))
    function jac_func(UÌ‚áµ¢, TÌ‚áµ¢, Î³áµ¢áµ¢)
        f.Wfact(W, UÌ‚áµ¢, p, dt * Î³áµ¢áµ¢, TÌ‚áµ¢)
        !isnothing(f.tgrad) && f.tgrad(fÌ‡, UÌ‚áµ¢, p, TÌ‚áµ¢)
    end
    function stage_func(::Val{i}) where {i}
        Î³áµ¢áµ¢ = diagÎ³ğŸ™[i]
        UÌ‚áµ¢ = i == 1 ? u : UÌ‚âºáµ¢
        TÌ‚áµ¢ = i == 1 ? t : t + dt * aÌ‚ğŸ™[i]

        run!(update_jac, update_jac_cache, NewStage(), jac_func, UÌ‚áµ¢, TÌ‚áµ¢, Î³áµ¢áµ¢)

        # Káµ¢ = (Î”t * Î³áµ¢áµ¢ * J - I)â»Â¹ * Î³áµ¢áµ¢ *
        #      (f(UÌ‚áµ¢, TÌ‚áµ¢) + âˆ‘_{j=1}^{i-1} (Î³â»Â¹)áµ¢â±¼ * Kâ±¼ + Î”t * fÌ‡ * âˆ‘_{j=1}^i Î³áµ¢â±¼)
        f(Ks[i], UÌ‚áµ¢, p, TÌ‚áµ¢)
        Ks[i] .= Î³áµ¢áµ¢ .* broadcasted(
            +,
            Ks[i],
            linear_combination(lowerÎ³â»Â¹[i], Ks)...,
            (isnothing(f.tgrad) ? () : (broadcasted(*, dt * Î³ğŸ™[i], fÌ‡),))...,
        )
        linsolve!(Ks[i], W, Ks[i]) # assume that linsolve! can handle aliasing

        # UÌ‚âºáµ¢ = u - Î”t * âˆ‘_{j=1}^i (aÌ‚ * Î³â»Â¹)áµ¢â±¼ * Kâ±¼
        UÌ‚âºáµ¢ .= broadcasted(+, u, scaled_linear_combination(aÌ‚Î³â»Â¹[i], Ks, -dt)...)
    end

    run!(update_jac, update_jac_cache, NewStep(), jac_func, u, t, diagÎ³ğŸ™[1])
    foreachval(stage_func, Val(num_stages(typeof(alg))))
    u .= UÌ‚âºáµ¢
end
